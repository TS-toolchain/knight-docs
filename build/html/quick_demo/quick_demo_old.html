

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>快速上手指南 &mdash; Knight_doc V3.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=eb26d1a0"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Knight_doc
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Knight 工具链</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../doc_info/doc_info.html">1. 修改记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/overview.html">2. 使用指南综述</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html">3. 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#id2">4. 开发环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#knight">5. Knight命令说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#id6">6. 模型转换配置文件示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#id7">7. 模型转换命令执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#id8">8. 仿真命令执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#id13">9. 板端运行</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_demo.html#id22">10. 功能特性说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides_base/quant.html">11. 量化使用指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides_base/compile.html">12. 编译仿真性能分析使用指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides_base/sdk.html">13. SDK使用指南</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/quant_faq.html">1. 量化工具FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op/op.html">2. 算子支持列表</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Knight_doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">快速上手指南</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_demo/quick_demo_old.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>快速上手指南<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h1>
<p>为便于客户更直观、更快速的了解清微智能骑士工具链 <code class="docutils literal notranslate"><span class="pre">TS.Knight</span></code> 的用法，将自己的业务神经网络模型快速的在清微智能芯片产品上部署，特针对工具链各模块（ <code class="docutils literal notranslate"><span class="pre">Knight</span></code> 量化工具、 <code class="docutils literal notranslate"><span class="pre">Knight</span>
<span class="pre">RNE</span></code> 编译器、 <code class="docutils literal notranslate"><span class="pre">Knigh</span> <span class="pre">RNE</span></code> 模拟器、 <code class="docutils literal notranslate"><span class="pre">Knight</span> <span class="pre">RNE</span></code> 性能分析器）提供结合语音、图像业务的快速上手用例。同时对这些用例进行讲解，以期帮助用户达到快速上手的目的。</p>
<p>这些用例结合具体业务只用于演示工具链的典型使用流程，使用的数据量较少，所以精度不具有参考价值，不代表工具链的实际精度效果。用例中的一些脚本，比如语音、图像数据的预处理，
客户可借鉴，如果有类似业务场景可在其基础上进行修改。客户如果想发挥出工具链的最大能力、更灵活的使用、应用到更复杂的场景，还需仔细阅读 <code class="docutils literal notranslate"><span class="pre">TS.Knight</span></code> 工具链各模块相应的使用指南文档。</p>
<p><strong>名词解释</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>名词</strong></p></th>
<th class="head"><p><strong>说明</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Knight</p></td>
<td><p>清微骑士工具链英文名称</p></td>
</tr>
<tr class="row-odd"><td><p>QAT</p></td>
<td><p>Quantization Aware
Training，量化感知训练</p></td>
</tr>
<tr class="row-even"><td><p>RNE</p></td>
<td><p>可重构神经网络加速引擎</p></td>
</tr>
<tr class="row-odd"><td><p>Finetune</p></td>
<td><p>微调</p></td>
</tr>
<tr class="row-even"><td><p>IR定点模型</p></td>
<td><p>中间表示模型，指
Caffe定点模型或ONNX定点模型</p></td>
</tr>
</tbody>
</table>
<section id="knight-demo">
<h2>Knight demo介绍<a class="headerlink" href="#knight-demo" title="Permalink to this heading"></a></h2>
<p>Knight demo是一系列基于智能语音、计算机视觉等领域的典型端到端应用sample，用来端到端的演示Knight工具链的使用流程和具体用法，覆盖Knight工具链的全部功能模块。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>说明：
Knight demo的定位是演示Knight工具链使用流程及用法，而非产品化的工程代码，不建议客户在产品中直接使用。尤其是业务的前后处理代码，仅服务于demo演示，未经过大规模数据测试。</p>
</div>
<section id="id2">
<h3>Knight demo整体介绍<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h3>
<p>整体分布如下：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quick_demo_img1.jpeg" />
</figure>
<p>具体demo如下（每一行为一个demo）：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>demo
编号</p></th>
<th class="head"><p>模型
名称</p></th>
<th class="head"><p>模型
框架</p></th>
<th class="head"><p>量化
平台</p></th>
<th class="head"><p>中间
IR</p></th>
<th class="head"><p>业务场景</p></th>
<th class="head"><p>Demo目的</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>yolov5</p></td>
<td><p>Pytorch</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>目标检测</p></td>
<td><p>演示Pytorch复杂业务模型量化功能</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>yolov5</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>目标检测</p></td>
<td><p>演示ONNX复杂业务模型量化功能</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>resnet18</p></td>
<td><p>Pytorch</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>图像分类</p></td>
<td><p>演示Pytorch模型使用ONNX量化工具功能</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>resnet18</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>图像分类</p></td>
<td><p>演示ONNX模型基础量化功能</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>resnet18</p></td>
<td><p>Caffe</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>图像分类</p></td>
<td><p>演示Caffe模型使用ONNX量化工具功能</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>resnet18</p></td>
<td><p>TF</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>图像分类</p></td>
<td><p>演示TF模型使用ONNX量化工具功能</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>resnet18</p></td>
<td><p>Paddle</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>图像分类</p></td>
<td><p>演示Paddle模型使用ONNX量化工具功能</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>gru</p></td>
<td><p>TF</p></td>
<td><p>ONNX</p></td>
<td><p>ONNX</p></td>
<td><p>语音检测</p></td>
<td><p>演示TF的语音检测模型的量化功能</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>resnet18_ops</p></td>
<td><p>Caffe</p></td>
<td><p>Caffe</p></td>
<td><p>Caffe</p></td>
<td><p>图像分类</p></td>
<td><p>演示CaffeIR自定义算子功能</p></td>
</tr>
</tbody>
</table>
<p>当前各系列芯片支持的demo如下：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>芯片型号</p></th>
<th class="head"><p>支持demo编号</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TX510x</p></td>
<td><p>1~8</p></td>
</tr>
<tr class="row-odd"><td><p>TX5336x-TX5256x</p></td>
<td><p>1~8</p></td>
</tr>
<tr class="row-even"><td><p>TX5368x_TX5339x_TX5335x</p></td>
<td><p>1~9</p></td>
</tr>
<tr class="row-odd"><td><p>TX5215x_TX5239x200_TX5239x220_TX5239x300</p></td>
<td><p>1~8</p></td>
</tr>
<tr class="row-even"><td><p>TX5112x_TX5239x201</p></td>
<td><p>1~8</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意：本文档仅以TX5368AV200为例进行demo演示，如需演示其他芯片demo, 仅修改–chip参数即可。</p>
</div>
</section>
<section id="id3">
<h3>Knight demo目录介绍<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h3>
<section id="dockerdemo">
<h4>docker镜像内demo目录介绍<a class="headerlink" href="#dockerdemo" title="Permalink to this heading"></a></h4>
<p>在产品发布包中提供了Knight镜像，进入镜像后在目录/TS-KnightDemo下存放的是Knight的示例，主目录结构如下图所示:</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_2.png" />
</figure>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>一级目录</p></th>
<th class="head"><p>二级目录</p></th>
<th class="head"><p>简介</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="6"><p>Resources</p></td>
<td><p>Classification</p></td>
<td><dl class="simple">
<dt>图像分类场景demo的模型，数据和代码</dt><dd><ul class="simple">
<li><p>data 目录存放量化数据和测试数</p></li>
<li><p>resnet18 目录存放resnet18模型</p></li>
<li><p>pysrc目录下存放python相关代码</p></li>
<li><p>csrc目录下存放C语言相关代码</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>ObjectDetection</p></td>
<td><dl class="simple">
<dt>图像检测场景demo的模型，数据和代码</dt><dd><ul class="simple">
<li><p>data 目录存放量化数据和测试数据</p></li>
<li><p>yolov5 目录存放yolov5模型</p></li>
<li><p>pysrc目录下存放python相关代码</p></li>
<li><p>csrc目录下存放C语言相关代码</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
<tr class="row-even"><td><p>Asr</p></td>
<td><dl class="simple">
<dt>语音检测场景demo的模型，数据和代码</dt><dd><ul class="simple">
<li><p>data 目录存放量化数据和测试数据</p></li>
<li><p>gru 目录存放gru模型</p></li>
<li><p>pysrc目录下存放python相关代码</p></li>
<li><p>csrc目录下存放C语言相关代码</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>RNESimLibForDemo</p></td>
<td><p>Knight demo 依赖的RNE模拟库</p></td>
</tr>
<tr class="row-even"><td><p>FinetuneLib</p></td>
<td><p>量化QAT demo示例，参见章节   <a class="reference internal" href="#finetunedemo">finetune库demo</a></p></td>
</tr>
<tr class="row-odd"><td><p>KnightMC</p></td>
<td><p>KnightMC模型压缩demo示例，参见章节 <a class="reference internal" href="#knight-mcdemo">knight-mc库demo</a></p></td>
</tr>
<tr class="row-even"><td><p>Scripts</p></td>
<td><p>yolov5_pytorch_chipAll.sh</p>
<p>yolov5_onnx_chipAll.sh
… …</p>
</td>
<td><blockquote>
<div><dl class="simple">
<dt>Scripts目录下，每个shell脚本对应一个demo。 命名规则如下：</dt><dd><ul class="simple">
<li><p>modelname_modelframework_chipAll.sh</p></li>
<li><p>modelname表示模型名称；</p></li>
<li><p>modelframework表示浮点模型的框架；</p></li>
<li><p>chipAll表示该模型所有芯片均支持，假如仅正在某几款芯片上支持该demo, 则
名称后缀改为chipCDEF，代表支持4款对应芯片。</p></li>
</ul>
</dd>
</dl>
</div></blockquote>
<p>比如 yolov5_pytorch_chipAll.sh对应demo1, 表示浮点模型yolov5，原始模型框架
为pytorch，使用ONNX 量化工具量化，且在所有芯片都支持。</p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="id4">
<h4>docker镜像外demo目录介绍<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h4>
<p>RNE运行时库的示例在容器外交付目录，对应不同芯片分别为</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">/TX510x-Lib</span></code>
<code class="docutils literal notranslate"><span class="pre">/TX5336x-TX5256x-Lib</span></code>
<code class="docutils literal notranslate"><span class="pre">/TX5368x_TX5339X_TX5335X-Lib</span></code>
<code class="docutils literal notranslate"><span class="pre">/TX5215x_TX5239x200_TX5239x220_TX5239x300-Lib</span></code>
<code class="docutils literal notranslate"><span class="pre">/TX5112x_TX5239x201-Lib</span></code></p>
</div></blockquote>
<p>其中 <code class="docutils literal notranslate"><span class="pre">RNE-RT-Lib_xxxx.tar.gz</span></code> 为 <code class="docutils literal notranslate"><span class="pre">RNE</span></code> 运行时库使用。</p>
</section>
</section>
<section id="id5">
<h3>Knight demo运行方式<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h3>
<p>Knight快速上手指南提供两种运行方式：一种是通过Knight
demo命令行方式运行演示；二是可通过demo脚本运行。</p>
<section id="demo">
<h4>通过demo命令行运行<a class="headerlink" href="#demo" title="Permalink to this heading"></a></h4>
<p>在启动容器后，输入</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>demo<span class="w"> </span>-h
</pre></div>
</div>
<p>界面示例如下图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_help.png" />
</figure>
<p></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>请注意，当 <code class="docutils literal notranslate"><span class="pre">--framework</span></code> 为不同量化框架时，可演示的模型范围有所不同。</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数名称</p></th>
<th class="head"><p>必需/
可选</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>说明
说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-f或–framework</p></td>
<td><p>必需</p></td>
<td><p>无</p></td>
<td><p>表示原始模型框架类型，可选范围{pytorch, tf, onnx, paddle, caffe}</p></td>
</tr>
<tr class="row-odd"><td><p>-m或–model-name</p></td>
<td><p>必需</p></td>
<td><p>无</p></td>
<td><p>表示当前demo中的模型名称。</p></td>
</tr>
<tr class="row-even"><td><p>-s或–step</p></td>
<td><p>可选</p></td>
<td><p>all</p></td>
<td><dl class="simple">
<dt>表示demo演示的阶段，该参数可选，默认all，取值范围{quant, rne, rne-sim-lib,all}：</dt><dd><ul class="simple">
<li><p>quant表示对demo模型进行量化，同时会对原始浮点模型进行推理测试、对量化后定点模型进行推理测试</p></li>
<li><p>rne表示对量化后的demo模型进行编译、模拟推理、性能分析。</p></li>
<li><p>rne-sim-lib表示对已经开发好的C代码app进行编译链接模拟库并运行。</p></li>
<li><p>all 表示顺序运行上述quantrnerne-sim-lib全流程。</p></li>
</ul>
</dd>
</dl>
<p>注意，需要先运行quant后，才可运行rne， rne运行后，才可运行rne-sim-lib。</p>
</td>
</tr>
<tr class="row-odd"><td><p>-h或–help</p></td>
<td><p>可选</p></td>
<td><p>无</p></td>
<td><p>显示帮助信息。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id6">
<h4>通过demo脚本运行<a class="headerlink" href="#id6" title="Permalink to this heading"></a></h4>
<p>Knight
demo示例也可通过Knight中demo脚本运行，具体脚本可参见 <a class="reference internal" href="#id12">模型部署资源生成快速指南</a> 中重要步骤说明。</p>
</section>
</section>
</section>
<section id="id7">
<h2>开发流程<a class="headerlink" href="#id7" title="Permalink to this heading"></a></h2>
<section id="ai">
<h3>AI全栈应用开发流程<a class="headerlink" href="#ai" title="Permalink to this heading"></a></h3>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_3.png" />
</figure>
<p></p>
<p>Knight工具链支持端侧AI应用全栈开发，包括应用开发，模型部署资源生成和自定义算子开发三个主要流程。</p>
<p>应用开发：用户调用Knight RNE SDK
API编写自己的业务应用，C代码加载模型部署资源，链接模拟库在纯软件环境中仿真调试自己的应用，没问题后，链接板端库在板端进行部署。</p>
<p>模型部署资源生成：用户准备已训练好的浮点模型，使用Knight
量化工具量化成IR定点模型，然后对比量化精度，接着编译生成模型部署资源，进行模拟器结果验证以及Profiling性能调优。详见 <a class="reference internal" href="#id12">模型部署资源生成快速指南</a> 。</p>
<p>自定义算子开发：当用户模型中存在芯片不支持的算子时，用户在量化后的IR模型中添加自定义算子层，之后进行IR模型编译生成模型部署资源；用户在应用开发时进行自定义算子的C代码实现，通过SDK
API相应接口进行自定义算子注册。最后，与整个应用程序一起进行模拟库上调测，板端库上部署。</p>
<p>浮点模型训练：用户在使用Knight工具链之前，需准备好已训练的浮点模型。</p>
</section>
<section id="id8">
<h3>模型部署资源生成开发流程<a class="headerlink" href="#id8" title="Permalink to this heading"></a></h3>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_4.png" />
</figure>
<p></p>
<ol class="arabic simple">
<li><p>用户使用Knight量化工具将提前训练好的浮点模型量化成IR定点模型。Knight ONNX量化工具demo请参见  <a class="reference internal" href="#quant">Quant</a> 。</p></li>
<li><p>用户使用Knight RNE编译器将IR定点模型编译成芯片部署资源(cfg和weight资源)。
Knight RNE编译器demo示例请参见 <a class="reference internal" href="#rne">RNE</a> 。</p></li>
<li><p>用户使用Knight RNE模拟器对测试数据进行推理，也可以使用Knight
RNE性能分析工具对模型进行性能分析。
Knight RNE模拟器demo请参见 <a class="reference internal" href="#rne">RNE</a> 。</p></li>
<li><p>同时用户也可以调用Knight
RNE模拟库编写自己的业务应用在纯软件环境仿真自己的业务模型。Knight
RNE模拟库demo请参见 <a class="reference internal" href="#id40">模拟库</a> 。</p></li>
<li><p>如果步骤3、4均通过，用户可以调用Knight
RNE运行时库编写自己的实际业务应用，部署到清微芯片上。
Knight RNE运行时库demo请参见 <a class="reference internal" href="#id51">运行时库</a>  ，板端部署示例请参见   <a class="reference internal" href="#id54">板端环境搭建及部署</a> 。</p></li>
<li><p>在步骤3中，如果模型推理性能不满足需求，则用户可使用Knight压缩工具（简称Knight-MC）将提前训练好的浮点模型进行压缩，得到体积更小，性能更优，更适合端侧部署的浮点模型。（可选）
Knight-MC demo示例请参见 <a class="reference internal" href="#knight-mcdemo">Knight-MC库demo</a> 。</p></li>
<li><p>在步骤1量化后，如果模型精度损失严重，用户可以使用QAT库，即Knight
Finetune库编写自己的Finetune工具对浮点模型进行微调，得到更适合量化的浮点模型，之后再进行步骤1。（可选）
Finetune 库demo示例请参见  <a class="reference internal" href="#finetunedemo">Finetune库demo</a> 。</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>在整个开发流程中有如下4个检查点：</p>
<ol class="arabic simple">
<li><p>用户使用Knight量化工具完成量化操作后，需要使用精度比对工具查看量化后精度是否满足业务要求；</p></li>
<li><p>用户使用Knight RNE模拟器对测试数据进行推理后，需保证其推理结果和Knight量化工具推理结果一致；</p></li>
<li><p>用户使用Knight RNE模拟库对测试数据进行推理后，需保证其推理结果和Knight RNE模拟器推理结果一致；</p></li>
<li><p>用户使用Knight RNE运行时库对测试数据进行推理后，需保证其推理结果和Knight RNE模拟库推理结果一致；</p></li>
</ol>
<p>以上4个检查点若不满足预期，可联系清微技术人员进行支持。</p>
<p>为便于用户快速进行检查点2，3的结果验证，提供model_check.py脚本，可参考 <a href="#id65"><span class="problematic" id="id66">`model_check.py使用说明`_</span></a> 。</p>
</div>
</section>
<section id="id9">
<h3>开发流程示例<a class="headerlink" href="#id9" title="Permalink to this heading"></a></h3>
<p>以图像分类业务和目标检测业务为例，说明Knight工具链在此类场景中的开发流程示例。</p>
<section id="id10">
<h4>目标检测场景<a class="headerlink" href="#id10" title="Permalink to this heading"></a></h4>
<p>目标检测场景的开发流程如下图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_5.png" />
</figure>
<p></p>
<ol class="loweralpha simple">
<li><dl class="simple">
<dt>浮点模型推理（用户已有业务流程）</dt><dd><ul class="simple">
<li><p>用户对测试图片进行前处理，通过原始浮点模型得到浮点推理结果。然后进行后处理（比如NMS，非极大值抑制算法）等操作并绘制检测框，得到最终的目标检测结果。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>浮点模型使用Knight工具链进行量化，编译，模拟推理</dt><dd><ul class="simple">
<li><p>用户对量化图片进行前处理，使用Knight量化工具得到IR模型，接着通过Knight编译器得到cfg/weight资源。</p></li>
<li><p>用户准备测试图片进行同样的前处理操作，此时根据需要转化成量化模型所需的数据类型，保存为bin文件，和cfg/weight资源文件一并输入Knight RNE 模拟器得到定点推理结果。</p></li>
<li><p>用户需要将定点推理结果进行反量化和后处理绘制检测框得到目标检测结果。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>板端推理</dt><dd><ul class="simple">
<li><p>用户使用相同的测试图片，进行相同的图像前处理并转化为所需的dtype类型，然后使用RNE-RT-Lib将cfg/weight资源进行板端部署，得到定点推理结果。</p></li>
<li><p>用户需要将定点推理结果进行反量化和后处理绘制检测框得到和图(b)中相同的目标检测结果。</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</section>
<section id="id11">
<h4>图像分类场景<a class="headerlink" href="#id11" title="Permalink to this heading"></a></h4>
<blockquote>
<div><p>图像分类场景的开发流程如下图所示：</p>
</div></blockquote>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_6.png" />
</figure>
<ol class="loweralpha simple">
<li><dl class="simple">
<dt>浮点模型推理（用户已有业务流程）</dt><dd><ul class="simple">
<li><p>用户对测试图片进行前处理，通过原始浮点模型得到浮点推理结果。若模型最后一层是softmax，输出为不同类别的概率，此时使用argmax取最大值，则可得到图像分类结果；若模型最后一层是argmax,则可直接得到图像分类结果。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>浮点模型使用Knight工具链进行量化，编译，模拟推理</dt><dd><ul class="simple">
<li><p>用户对量化图片进行前处理，使用Knight量化工具得到IR模型，接着通过Knight编译器得到cfg/weight资源。</p></li>
<li><p>用户准备测试图片进行同样的前处理操作，此时根据需要转化成量化模型所需的dtype数据类型。比如：图片前处理中最后一步是减均值除方差操作，则前处理后数据类型为浮点。在使用量化工具时，若–ir-input-dtype指定为float32（默认），则此处的dtype为浮点类型，无需进行转换；若–ir-input-dtype指定为int8，则此处的dtype为int8，需要将浮点数据转换为int8类型。</p></li>
<li><p>接着将dtype类型数据保存为bin文件，和cfg/weight资源文件一并输入Knight RNE 模拟器得到定点推理结果，从而得到图像分类结果。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>板端推理</dt><dd><ul class="simple">
<li><p>用户使用相同的测试图片，进行相同的图像前处理并转化为所需的dtype类型，然后使用RNE-RT-Lib将cfg/weight资源进行板端部署，得到和图(b)中相同的定点推理结果。</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</section>
</section>
</section>
<section id="id12">
<h2>模型部署资源生成快速指南<a class="headerlink" href="#id12" title="Permalink to this heading"></a></h2>
<p>在docker
容器下运行快速上手用例，需要进行docker环境准备，并运行容器，详细安装及使用步骤请参阅  <a class="reference internal" href="../overview/overview.html"><span class="doc">使用指南综述</span></a> 。</p>
<section id="quant">
<h3>Quant<a class="headerlink" href="#quant" title="Permalink to this heading"></a></h3>
<section id="id13">
<h4>命令行运行方式<a class="headerlink" href="#id13" title="Permalink to this heading"></a></h4>
<p>Quant表示对demo模型进行浮点推理，量化以及量化后定点模型推理测试，命令如下所示：
以demo1为例演示onnx quant 流程</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>demo<span class="w"> </span>-f<span class="w"> </span>pytorch<span class="w"> </span>-m<span class="w"> </span>yolov5<span class="w"> </span>--step<span class="w"> </span>quant
</pre></div>
</div>
</section>
<section id="id14">
<h4>脚本运行方式<a class="headerlink" href="#id14" title="Permalink to this heading"></a></h4>
<p>demo1对应的shell脚本为 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Scripts/yolov5_pytorch_chipAll.sh</span></code> ，以yolov5 pytorch浮点模型使用ONNX量化工具为例，运行方式如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>/TS-KnightDemo/Scripts/yolov5_pytorch_chipAll.sh<span class="w"> </span>TX5368AV200<span class="w"> </span>quant
</pre></div>
</div>
</section>
<section id="id15">
<h4>重要步骤<a class="headerlink" href="#id15" title="Permalink to this heading"></a></h4>
<section id="id16">
<h5>原始浮点模型推理过程<a class="headerlink" href="#id16" title="Permalink to this heading"></a></h5>
<p>在使用Knight量化工具之前，需要用户准备好训练充分的浮点模型。基于官方开源的yolov5项目进行图像检测，命令行如下所示：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/TS-KnightDemo/Resources/ObjectDetection/pysrc/yolov5_master
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">detect</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">source</span> <span class="o">/</span><span class="n">TS</span><span class="o">-</span><span class="n">KnightDemo</span><span class="o">/</span><span class="n">Resources</span><span class="o">/</span><span class="n">ObjectDetection</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">test_data</span><span class="o">/</span><span class="n">bus</span><span class="o">.</span><span class="n">jpg</span> <span class="o">--</span><span class="n">weights</span> <span class="n">yolov5s</span><span class="o">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">img</span> <span class="mi">640</span>
</pre></div>
</div>
<p>执行成功后会对source指向的图片进行目标检测，并在runs/detect/expN目录中输出以下两个文件：</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>expN中的N为变数，会随着推理次数增加而递增</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bus.jpg</span></code>：绘制了检测框的图片；
<code class="docutils literal notranslate"><span class="pre">yolo_result.txt</span></code>：保存了检测框信息的文件。</p>
<p>完整的浮点模型推理包含以下3个步骤：</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt><strong>前处理</strong></dt><dd><p>前处理过程一般包括resize调整大小、reshape矩阵转换、减均值除方差标准化等预处理。量化数据、测试数据均遵循此方法。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>模型推理</strong></dt><dd><p>预处理后的数据输入到原始浮点模型中，推理得到浮点输出结果。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>后处理</strong></dt><dd><p>将浮点输出结果进行边界框回归和非极大值抑制等后处理操作，得到最终目标检测框的坐标，格式为[x, y, w, h, conf, label]，保存在文本 <code class="docutils literal notranslate"><span class="pre">.txt</span></code> 中，并在图像中绘制检查框得到bus.jpg。</p>
</dd>
</dl>
</li>
</ol>
</section>
<section id="id17">
<h5>量化过程<a class="headerlink" href="#id17" title="Permalink to this heading"></a></h5>
<p><strong>ONNX 量化</strong></p>
<p>ONNX量化工具内集成了TF2ONNX, Caffe2ONNX, Pytorch2ONNX,Paddle2ONNX转换工具，不仅支持ONNX 浮点模型的量化还可支持Tensorflow, Caffe, Pytorch和PaddlePadlle浮点模型的转换和量化。此处以yolov5 pytorch浮点模型使用ONNX量化工具为例进行说明。</p>
<ul>
<li><p>数据预处理</p>
<blockquote>
<div><p>Infer推理函数存放在目录 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts</span></code> 中，调用 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py</span></code> 中的预处理函数</p>
</div></blockquote>
</li>
</ul>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_7.png" />
</figure>
<p></p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_8.png" />
</figure>
<p></p>
<p>ONNX 量化使用转换命令和量化分步完成：</p>
<ul class="simple">
<li><p><strong>模型转换</strong></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>yolov5<span class="w"> </span>-f<span class="w"> </span>pytorch<span class="w"> </span>-r<span class="w"> </span>convert<span class="w"> </span>-w
/TS-KnightDemo/Resources/ObjectDetection/yolov5/yolov5s.pt<span class="w"> </span>-s
/TS-KnightDemo/Output/yolov5_pytorch/quant<span class="w"> </span>-uds
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
-l<span class="w"> </span><span class="m">3</span>
</pre></div>
</div>
<ul class="simple">
<li><dl class="simple">
<dt><strong>模型量化</strong></dt><dd><p>量化转换后的yolov5模型：</p>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#命令</span>
Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--run-config
/TS-KnightDemo/Scripts/yolov5_config.json<span class="w"> </span>-m
/TS-KnightDemo/Output/yolov5_pytorch/quant/yolov5.onnx<span class="w"> </span>-f<span class="w"> </span>onnx<span class="w"> </span>-if
infer_yolov5<span class="w"> </span>--save-dir<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/quant<span class="w"> </span>-d
/TS-KnightDemo/Resources/ObjectDetection/data/quant_data/coco/images/val2017
-uds
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
-qid<span class="w"> </span>uint8<span class="w"> </span>--dump
</pre></div>
</div>
<dl>
<dt>量化结果：</dt><dd><blockquote>
<div><p>量化后的模型保存在 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Output/yolov5_pytorch/quant</span></code> 文件夹下， <code class="docutils literal notranslate"><span class="pre">yolov5_quantize.onnx</span></code> 是量化后的模型文件。在 <code class="docutils literal notranslate"><span class="pre">yolov5</span></code> 的demo中，除了提供一个基础ONNX浮点模型做演示外，也额外提供了一个推理时间更短的ONNX模型做演示，这个模型是使用relu作为激活层的。</p>
</div></blockquote>
<p>具体实现可参考脚本/TS-KnightDemo/Scripts/yolov5_onnx_chipAll.sh，其中定义了一个变量use_relu_model，使用者可根据自身需要通过改变变量的取值来完成两个模型的使用切换。
此处以yolov5ONNX的两个浮点模型使用ONNX量化工具生成子图为例，进行量化部分区别的说明。</p>
</dd>
</dl>
<ul class="simple">
<li><dl class="simple">
<dt><strong>子模型生成</strong></dt><dd><p>Onnx浮点模型在量化前，需要将模型里不支持的算子去掉重新保存为子模型，再进行下一步的量化。使用模型yolov5s_v7.0.onnx生成子模型：</p>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#命令</span>
Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--run-config
/TS-KnightDemo/Scripts/yolov5_config.json<span class="w"> </span>-m
/TS-KnightDemo/Resources/ObjectDetection/yolov5/yolov5s_v7.0.onnx<span class="w"> </span>-f
onnx<span class="w"> </span>-if<span class="w"> </span>infer_yolov5<span class="w"> </span>--save-dir
/TS-KnightDemo/Output/yolov5_onnx/quant<span class="w"> </span>-d
/TS-KnightDemo/Resources/ObjectDetection/data/quant_data/coco/images/val2017
-uds
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_torch/yolov5_onnx.py
-qid<span class="w"> </span>uint8<span class="w"> </span>--dump<span class="w"> </span>--output-name<span class="w"> </span>/model.24/m.0/Conv_output_0
/model.24/m.1/Conv_output_0<span class="w"> </span>/model.24/m.2/Conv_output_0
</pre></div>
</div>
<p>使用relu激活的模型yolov5s_v7.0_relu.onnx生成子模型：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#命令</span>
<span class="c1">#使用relu激活的模型yolov5s_v7.0_relu.onnx</span>
Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--run-config
/TS-KnightDemo/Scripts/yolov5_config.json<span class="w"> </span>-m
/TS-KnightDemo/Resources/ObjectDetection/yolov5/yolov5s_v7.0_relu.onnx
-f<span class="w"> </span>onnx<span class="w"> </span>-if<span class="w"> </span>infer_yolov5<span class="w"> </span>--save-dir
/TS-KnightDemo/Output/yolov5_onnx/quant<span class="w"> </span>-d
/TS-KnightDemo/Resources/ObjectDetection/data/quant_data/coco/images/val2017
-uds
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_torch/yolov5_onnx.py
-qid<span class="w"> </span>uint8<span class="w"> </span>--dump<span class="w"> </span>--output-name<span class="w"> </span><span class="m">269</span><span class="w"> </span><span class="m">324</span><span class="w"> </span><span class="m">379</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意：
在量化命令中有2点注意事项：
1. 指定–std,–mean,将标准化操作融合到模型中，可加速应用在芯片上的推理性能，同时需指定量化后定点模型输入数据类型uint8,即-qid uint8;
2. 量化后定点模型推理得到定点结果，需要进行反量化操作，才可进行后处理。若指定-od,
则量化时自动在定点模型中追加反量化层，定点模型推理后无需再进行反量化。</p>
</div>
</section>
<section id="id18">
<h5>量化后定点模型推理过程<a class="headerlink" href="#id18" title="Permalink to this heading"></a></h5>
<p><strong>ONNX 量化</strong></p>
<ul>
<li><p>定点推理</p>
<blockquote>
<div><p>ONNX量化后定点模型推理命令如下所示:</p>
</div></blockquote>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--run-config
/TS-KnightDemo/Scripts/yolov5_config.json<span class="w"> </span>-r<span class="w"> </span>infer<span class="w"> </span>-d
/TS-KnightDemo/Resources/ObjectDetection/data/test_data/bus.jpg<span class="w"> </span>-m
/TS-KnightDemo/Output/yolov5_pytorch/quant/yolov5_quantize.onnx<span class="w"> </span>-f<span class="w"> </span>onnx
--save-dir<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/quant<span class="w"> </span>-uds
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
</pre></div>
</div>
<p>推理结果：推理后的结果保存在 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Output/yolov5_pytorch/quant</span></code> 文件夹下，result_0_p.txt, result_1_p.txt, result_2_p.txt是推理后的结果文件。</p>
<ul>
<li><p>后处理</p>
<blockquote>
<div><p>量化后定点模型推理指令中，也在内部调用了后处理操作，结果保存为 <code class="docutils literal notranslate"><span class="pre">yolov5_result.jpg</span></code></p>
</div></blockquote>
</li>
<li><p><strong>MAP示范</strong></p>
<blockquote>
<div><p>在yolov5的量化过程中，我们使用了多张图片做为量化数据集，在模型量化后，也提供了批量推理图片并展示MAP指标的步骤。ONNX定点模型批量推理命令如下所示：</p>
</div></blockquote>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-r<span class="w"> </span>infer<span class="w"> </span>-m
/TS-KnightDemo/Output/yolov5_pytorch/quant/yolov5_quantize.onnx<span class="w"> </span>-f<span class="w"> </span>onnx
-if<span class="w"> </span>infer_yolov5<span class="w"> </span>-s<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/quant<span class="w"> </span>-d
/TS-KnightDemo/Resources/ObjectDetection/data/quant_data/coco/images/val2017
-uds<span class="w"> </span>/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
</pre></div>
</div>
<p>MAP的参数如图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_9.png" />
</figure>
<p></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意：在量化时指定-od，此处定点模型推理后无需再进行反量化。</p>
</div>
</section>
<section id="id19">
<h5>检查点<a class="headerlink" href="#id19" title="Permalink to this heading"></a></h5>
<p>此时可进行检查点1的检验，对比原始浮点模型的目标检测结果和ONNX量化后的目标检查结果。
Pytorch yolov5原始浮点模型推理后的结果为：</p>
<p><code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/ObjectDetection/pysrc/yolov5_master/runs/detect/expN/bus.jpg</span></code></p>
<p>经过ONNX量化后的结果为： <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Output/yolov5_pytorch/quant/yolov5_result.jpg</span></code></p>
<p>若不满足，可使用精度对比工具  <a class="reference internal" href="#compare">compare</a> 进行问题定位。</p>
</section>
</section>
</section>
<section id="rne">
<h3>RNE<a class="headerlink" href="#rne" title="Permalink to this heading"></a></h3>
<section id="id20">
<span id="id21"></span><h4>命令行运行方式<a class="headerlink" href="#id20" title="Permalink to this heading"></a></h4>
<p>RNE表示对量化后的demo模型进行RNE编译器编译、RNE模拟器推理、RNE性能分析器评估性能。RNE demo命令如下所示：</p>
<p>以demo1为例演示RNE流程</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>demo<span class="w"> </span>-f<span class="w"> </span>pytorch<span class="w"> </span>-m<span class="w"> </span>yolov5<span class="w"> </span>--step<span class="w"> </span>rne
</pre></div>
</div>
</section>
<section id="id22">
<span id="id23"></span><h4>脚本运行方式<a class="headerlink" href="#id22" title="Permalink to this heading"></a></h4>
<p>以yolov5
pytorch浮点模型使用ONNX量化工具为例，在目录 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Scripts/yolov5_pytorch_chipAll.sh</span></code> 中RNE阶段对应的脚本如下所示：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#命令</span>
bash<span class="w"> </span>/TS-KnightDemo/Scripts/yolov5_pytorch_chipAll.sh<span class="w"> </span>TX5368AV200<span class="w"> </span>rne
</pre></div>
</div>
</section>
<section id="id24">
<span id="id25"></span><h4>重要步骤<a class="headerlink" href="#id24" title="Permalink to this heading"></a></h4>
<section id="id26">
<h5>RNE编译器<a class="headerlink" href="#id26" title="Permalink to this heading"></a></h5>
<p>RNE编译命令如下所示：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>compile<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>--onnx
/TS-KnightDemo/Output/yolov5_pytorch/quant/yolov5_quantize.onnx
--save-dir<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/rne
</pre></div>
</div>
</section>
<section id="id27">
<h5>RNE模拟器<a class="headerlink" href="#id27" title="Permalink to this heading"></a></h5>
<p>在模拟器上跑编译后的模型仿真</p>
<ol class="arabic simple">
<li><p><strong>测试图像经过前处理得到.bin</strong></p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
--input<span class="w"> </span>/TS-KnightDemo/Resources/ObjectDetection/data/test_data/bus.jpg
--outpath<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/rne<span class="w"> </span>--pre_processing
</pre></div>
</div>
<p>执行成功后会输出文件： <code class="docutils literal notranslate"><span class="pre">model_input.bin</span></code> 文件，可作为模拟器的输入使用。</p>
<ol class="arabic simple" start="2">
<li><p><strong>使用模拟器推理一条测试数据</strong></p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>run<span class="w"> </span>--input
/TS-KnightDemo/Output/yolov5_pytorch/rne/model_input.bin<span class="w"> </span>--model
/TS-KnightDemo/Output/yolov5_pytorch/rne/yolov5_quantize_r.tsmodel
--save-dir<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/rne<span class="w"> </span>-fmt<span class="w"> </span>nchw
</pre></div>
</div>
<dl class="simple">
<dt>执行该命令后，得到以下三路输出文件：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">result-357_p.txt</span></code>， <code class="docutils literal notranslate"><span class="pre">result-358_p.txt</span></code>， <code class="docutils literal notranslate"><span class="pre">result-359_p.txt</span></code>，维度分别为[1, 255, 80, 80]，[1, 255, 40, 40]，[1, 255, 20, 20]，这三个文件中保存了模拟器定点的输出数据。</p>
</dd>
</dl>
<ol class="arabic" start="3">
<li><p><strong>模拟器输出结果后处理</strong></p>
<p>将模拟器输出的三个结果文件result-357_p.txt，result-358_p.txt，result-359_p.txt读入，做反量化及NMS等后处理后，获得预测框信息。</p>
</li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
--input<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/rne<span class="w"> </span>--outpath
/TS-KnightDemo/Output/yolov5_pytorch/rne<span class="w"> </span>--proc-mode<span class="w"> </span>pytorch_use_onnx
--post_processing<span class="w"> </span>--data
/TS-KnightDemo/Resources/ObjectDetection/data/test_data/bus.jpg
</pre></div>
</div>
<p>执行完毕后，会在outpath指向的目录中输出下内容：
<code class="docutils literal notranslate"><span class="pre">yolov5_result.jpg</span></code>：检测结果图片。
<code class="docutils literal notranslate"><span class="pre">result.txt</span></code>：保存了检测框的文件。</p>
</section>
<section id="id28">
<h5>RNE性能分析器<a class="headerlink" href="#id28" title="Permalink to this heading"></a></h5>
<p>RNE Profiling 执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>profiling<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>--model
/TS-KnightDemo/Output/yolov5_pytorch/rne/yolov5_quantize_r.tsmodel
--save-dir<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/rne<span class="w"> </span>--log-level<span class="w"> </span><span class="m">3</span>
</pre></div>
</div>
</section>
<section id="id29">
<span id="id30"></span><h5>检查点<a class="headerlink" href="#id29" title="Permalink to this heading"></a></h5>
<p>使用模拟器推理完成后，需要进行检查3与检查点2的数据result-357_p.txt，result-358_p.txt，result-359_p.txt对比，两者结果应完全一致，参见<a class="reference external" href="#model_check">model_check</a>。</p>
</section>
</section>
</section>
<section id="model-check">
<h3>model_check<a class="headerlink" href="#model-check" title="Permalink to this heading"></a></h3>
<p>在执行完量化和编译的命令后可使用model_check.py进行检查点2和检查点3结果的验证，验证命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/TS-KnightSoftware/tools/model_check/

python<span class="w"> </span>model_check.py<span class="w"> </span>--quant-output
/TS-KnightDemo/Output/yolov5_pytorch/quant/dump<span class="w"> </span>--compile-output
/TS-KnightDemo/Output/yolov5_pytorch/rne<span class="w"> </span>--run-mode<span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_10.png" />
</figure>
<p></p>
</section>
<section id="compare">
<h3>compare<a class="headerlink" href="#compare" title="Permalink to this heading"></a></h3>
<p>在执行量化命令后，若发现结果存在不一致，可使用 <code class="docutils literal notranslate"><span class="pre">compare</span></code> 工具进行精度分析和比较，定位问题。首先需要执行 <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> 量化脚本, 并指定 <code class="docutils literal notranslate"><span class="pre">--dump</span></code> 模式</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m
/TS-KnightDemo/Output/yolov5_pytorch/quant/yolov5.onnx<span class="w"> </span>-f<span class="w"> </span>onnx<span class="w"> </span>-if
infer_yolov5<span class="w"> </span>-s<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/quant<span class="w"> </span>-bs<span class="w"> </span><span class="m">1</span><span class="w"> </span>-qm
min_max<span class="w"> </span>-d
/TS-KnightDemo/Resources/ObjectDetection/data/quant_data/coco128/images/train2017
-uds
/TS-KnightDemo/Resources/ObjectDetection/pysrc/TS_yolov5_onnx_from_ts/yolov5_onnx_ts.py
--mean<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span>--std<span class="w"> </span><span class="m">255</span>.0<span class="w"> </span><span class="m">255</span>.0<span class="w"> </span><span class="m">255</span>.0<span class="w"> </span>-qid<span class="w"> </span>uint8<span class="w"> </span>--dump
</pre></div>
</div>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_11.png" />
</figure>
<p></p>
<p>然后可执行精度比对工具</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#命令</span>
Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>compare<span class="w"> </span>-qd<span class="w"> </span>/TS-KnightDemo/Output/yolov5_pytorch/quant
</pre></div>
</div>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_12.png" />
</figure>
<p></p>
</section>
<section id="finetunedemo">
<h3>Finetune库demo<a class="headerlink" href="#finetunedemo" title="Permalink to this heading"></a></h3>
<section id="id31">
<h4>demo文件夹说明<a class="headerlink" href="#id31" title="Permalink to this heading"></a></h4>
<p>用例在容器内的路径:
<code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/FinetuneLib/cifar10_example</span></code>,目录结构如下：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>文件夹或文件</p></th>
<th class="head"><p>说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>checkpoint</p></td>
<td><blockquote>
<div><p>训练完成后生成该文件夹并存放训练的浮点模型</p>
</div></blockquote>
<p>ResNet18_ckpt.pth、Finetune后的模型
ResNet18_ckpt_q.pth</p>
</td>
</tr>
<tr class="row-odd"><td><p>data</p></td>
<td><p>存放Finetune所需的训练数据及测试数据</p></td>
</tr>
<tr class="row-even"><td><p>models</p></td>
<td><p>存放模型文件</p></td>
</tr>
<tr class="row-odd"><td><p>script</p></td>
<td><p>存放运行此用例的脚本,</p>
<p>其中train.sh为浮点模型训练脚本,
finetune.sh为浮点模型Finetune训练脚本,</p>
<p>test.sh为对浮点模型及Finetune模型进行测试的脚本,</p>
<p>run_e2e_example.sh为总执行入口脚本</p>
</td>
</tr>
<tr class="row-even"><td><p>main.py</p></td>
<td><p>Finetune主函数入口文件</p></td>
</tr>
<tr class="row-odd"><td><p>utils.py</p></td>
<td><p>Finetune运行时所依赖的函数文件</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id32">
<h4>demo运行说明<a class="headerlink" href="#id32" title="Permalink to this heading"></a></h4>
<p>执行如下命令：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/TS-KnightDemo/Resources/FinetuneLib/cifar10_example/script
sh<span class="w"> </span>run_e2e_example.sh
</pre></div>
</div>
<p>执行成功后，会看到以下信息：
步骤一：train.sh结果如下图：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_13.png" />
</figure>
<p></p>
<p>步骤二：finetune.sh结果如下图：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_14.png" />
</figure>
<p></p>
<p>步骤三：test.sh结果如下图：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_15.png" />
</figure>
<p></p>
<p>最终结果保存在 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/FinetuneLib/cifar10_example/checkpoint</span></code> 目录下</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>文件夹或文件</strong></p></th>
<th class="head"><p><strong>说明</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ResNet18_ckpt.pth</p></td>
<td><p>浮点模型文件。</p></td>
</tr>
<tr class="row-odd"><td><p>R
esNet18_ckpt_q.pth</p></td>
<td><p>Finetune后模型文件。</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>因为演示需要，浮点模型只训练了一个epoch，要想获得更好的浮点精度和Finetune精度可以在train.py里增加epoch数量。</p>
</div>
</section>
<section id="id33">
<h4>重要步骤说明<a class="headerlink" href="#id33" title="Permalink to this heading"></a></h4>
<p>在Knight镜像内/TS-KnightDemo/Resources/FinetuneLib/cifar10_example/main.py是Finetune-Lib运行的必须脚本，执行python
main.py -h 可以获得如下图所示的命令行参数：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_16.png" />
</figure>
<p></p>
<ol class="arabic simple">
<li><p><strong>训练浮点模型</strong></p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>ResNet18<span class="w"> </span>--epochs<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>执行train.sh，其中ResNet18为模型名称，epochs为epoch的数量，执行完成后会生成ResNet18_ckpt.pth</p>
<ol class="arabic simple" start="2">
<li><p><strong>Finetune浮点模型</strong></p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>ResNet18<span class="w"> </span>--resume<span class="w"> </span>--BackendType<span class="w"> </span>TSQAT<span class="w"> </span>--quantize<span class="w"> </span>--lr<span class="w"> </span>1e-5<span class="w"> </span>--epochs<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>执行finetune.sh，其中ResNet18为模型名称，BackendType
为后端的名称，epochs为epoch的数量，lr为学习率，–quantize使能Finetune功能，执行完成后会生成ResNet18_ckpt_q.pth</p>
<ol class="arabic simple" start="3">
<li><p><strong>测试原始浮点模型和Finetune浮点模型的精度</strong></p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>ResNet18<span class="w"> </span>--resume<span class="w"> </span>-e

python<span class="w"> </span>main.py<span class="w"> </span>ResNet18<span class="w"> </span>--resume<span class="w"> </span>--quantize<span class="w"> </span>-e
</pre></div>
</div>
<p>执行 <code class="docutils literal notranslate"><span class="pre">test.sh</span></code>，其中ResNet18为模型名称，resume为指定训练好的模型，–quantize使能Finetune功能，-e为仅仅进行模型测试而不做训练。</p>
<ol class="arabic" start="4">
<li><p><strong>编译量化后的模型</strong></p>
<p>参见 <a class="reference internal" href="#id26">RNE编译器</a></p>
</li>
<li><p><strong>在RNE模拟器上跑编译后的模型</strong></p>
<p>参见  <a class="reference internal" href="#id27">RNE模拟器</a></p>
</li>
</ol>
</section>
</section>
<section id="knight-mcdemo">
<h3>Knight-MC库demo<a class="headerlink" href="#knight-mcdemo" title="Permalink to this heading"></a></h3>
<p>Knight-MC库结合cifar10开源数据集提供一个图像分类业务的用例，网络结构是resnet18，模型输入为cifar10原始数据，输出结果为10分类的结果。</p>
<section id="demo-1">
<span id="id34"></span><h4>demo文件夹说明<a class="headerlink" href="#demo-1" title="Permalink to this heading"></a></h4>
<p>用例在容器内的路径: <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/</span></code> ,目录结构如下：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>文件夹或文件</strong></p></th>
<th class="head"><p><strong>说明</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dataset</p></td>
<td><p>存放所需的训练数据和测试数据</p></td>
</tr>
<tr class="row-odd"><td><p>pretrained_model</p></td>
<td><p>存放预训练的图像分类模型</p></td>
</tr>
<tr class="row-even"><td><p>resnet18_cifar</p></td>
<td><p>Resnet18图像分类模型压缩示例</p>
<p>pruning_
config.yaml：Pruning工具配置文件示例</p>
<p>sparsity
config.yaml：Sparsity工具配置文件示例</p>
<p>pruning_demo.py：调用Pruning工具代码示例</p>
<p>sparsity_demo.py：调用Sparsity工具代码示例</p>
<p>resnet_cifar.py: resnet模型结构定义</p>
<p>train_val.py:
训练模型代码，包含数据加载处理代码</p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="id35">
<span id="id36"></span><h4>demo运行说明<a class="headerlink" href="#id35" title="Permalink to this heading"></a></h4>
<p>Pruning Demo剪枝脚本执行如下命令：
工作路径 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/resnet18_cifar</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>pruning_demo.py
</pre></div>
</div>
<p>Sparsity Demo稀疏脚本执行如下命令：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 工作路径 /TS-KnightDemo/Resources/KnightMC/resnet18_cifar</span>
python<span class="w"> </span>sparsity_demo.py
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>因为演示需要，Pruning剪枝训练次数设置次数较小warmup: 10，num_heatup_episodes：10。</p>
</div>
<p>同时，稀疏或剪枝后模型重训练只训练了2个epoch，要想获得更好的浮点精度可以在pruning_demo.py或sparsity_demo.py里增加epoch数量。</p>
</section>
<section id="id37">
<span id="id38"></span><h4>重要步骤说明<a class="headerlink" href="#id37" title="Permalink to this heading"></a></h4>
<section id="pruning-demo">
<h5>Pruning Demo重要步骤说明<a class="headerlink" href="#pruning-demo" title="Permalink to this heading"></a></h5>
<ol class="arabic simple">
<li><p><strong>准备预训练的浮点模型</strong></p></li>
</ol>
<p>在示例中，已准备好预训练的浮点模型，在目录 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/pretrained_model</span></code> 中。同时用户自行训练得到浮点模型，训练脚本如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>train_val.py
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>配置yaml文件</strong></p></li>
</ol>
<p>若不结合Knight工具链，仅考虑模型精度，不考虑模型在芯片上的推理时间，则需要将配置文件pruning_config.yaml中参数run_knight设置为False。</p>
<ol class="arabic simple" start="3">
<li><p><strong>剪枝浮点模型和重训练浮点模型</strong></p></li>
</ol>
<p>执行如下脚本同时完成剪枝和重训练过程，若无需进行重训练则将代码中重训练部分进行屏蔽即可。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>pruning_demo.py
</pre></div>
</div>
<p>执行该脚本，剪枝后的模型保存在 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/resnet18_cifar/output</span></code> 目录下:</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_17.png" />
</figure>
<p></p>
<p>其中kmc-pruning.csv文件内容如下所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_18.png" />
</figure>
<p></p>
<p>重训练之后的模型保存在 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/resnet18_cifar/logs</span></code> 目录下</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_19.png" />
</figure>
<p></p>
<p>pruning_demo.py中包含了测试重训练模型精度的步骤，页面输入示例如下：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_20.png" />
</figure>
<ol class="arabic" start="4">
<li><p><strong>编译量化后的模型</strong></p>
<p>参见 <a class="reference internal" href="#id26">RNE编译器</a></p>
</li>
<li><p><strong>在RNE模拟器上跑编译后的模型</strong></p>
<p>参见 <a class="reference internal" href="#id27">RNE模拟器</a></p>
</li>
</ol>
</section>
<section id="sparsity-demo">
<h5>Sparsity Demo重要步骤说明<a class="headerlink" href="#sparsity-demo" title="Permalink to this heading"></a></h5>
<ol class="arabic simple">
<li><p><strong>准备预训练的浮点模型</strong></p></li>
</ol>
<p>在示例中，已准备好预训练的浮点模型，在目录 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/pretrained_model</span></code> 中。同时用户自行训练得到浮点模型，训练脚本如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>train_val.py
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>配置yaml文件</strong></p></li>
</ol>
<p>在配置文件 <code class="docutils literal notranslate"><span class="pre">sparsity_config.yaml</span></code> 中配置参数 <code class="docutils literal notranslate"><span class="pre">sparsity_method</span></code> 选择不同的稀疏方式。</p>
<ol class="arabic simple" start="3">
<li><p><strong>稀疏浮点模型和重训练浮点模型</strong></p></li>
</ol>
<p>执行如下脚本同时完成稀疏和重训练过程，若无需进行重训练则将代码中重训练部分进行屏蔽，同时增加保存稀疏后模型的代码即可。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sparsity_demo.py
</pre></div>
</div>
<p>执行该脚本，稀疏并重训练后模型后的模型保存在
<code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/KnightMC/resnet18_cifar/logs</span></code> 目录下：</p>
<blockquote>
<div><figure class="align-center">
<img alt="pipeline" src="../_images/demo_21.png" />
</figure>
</div></blockquote>
<p>sparsity_demo.py中包含了测试重训练模型精度的步骤，页面输入示例如下：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_22.png" />
</figure>
<ol class="arabic" start="4">
<li><p><strong>编译量化后的模型</strong></p>
<p>参见 <a class="reference internal" href="#id26">RNE编译器</a></p>
</li>
<li><p><strong>在RNE模拟器上跑编译后的模型</strong></p>
<p>参见 <a class="reference internal" href="#id27">RNE模拟器</a></p>
</li>
</ol>
</section>
</section>
</section>
</section>
<section id="id39">
<h2>应用开发快速指南<a class="headerlink" href="#id39" title="Permalink to this heading"></a></h2>
<section id="id40">
<h3>模拟库<a class="headerlink" href="#id40" title="Permalink to this heading"></a></h3>
<section id="id41">
<span id="id42"></span><h4>命令行运行方式<a class="headerlink" href="#id41" title="Permalink to this heading"></a></h4>
<p>模拟库demo表示对已经开发好的C代码app进行编译链接模拟库并运行，demo命令示例如下所示：
以demo1为例演示rne-sim-lib流程</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>demo<span class="w"> </span>-f<span class="w"> </span>pytorch<span class="w"> </span>-m<span class="w"> </span>yolov5<span class="w"> </span>--step<span class="w"> </span>rne-sim-lib
</pre></div>
</div>
</section>
<section id="id43">
<span id="id44"></span><h4>脚本运行方式<a class="headerlink" href="#id43" title="Permalink to this heading"></a></h4>
<p>以yolov5 pytorch浮点模型使用ONNX量化工具为例，在目录 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Scripts/yolov5_pytorch_chipAll.sh</span></code> 中rne-sim-lib阶段对应的脚本如下所示：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>/TS-KnightDemo/Scripts/yolov5_pytorch_chipAll.sh<span class="w"> </span>TX5368AV200<span class="w"> </span>rne-sim-lib
</pre></div>
</div>
</section>
<section id="id45">
<span id="id46"></span><h4>重要步骤<a class="headerlink" href="#id45" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>C代码开发</p></li>
</ol>
<p>编写 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/ObjectDetection/csrc/yolov5_simlib/src/main.cpp</span></code> 主要API接口说明
为了方便用户使用，模拟库和运行时库提供了一套C形式的API接口。</p>
<p>主要API接口说明如下：C接口说明（部分）：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>接口名称</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数名称及描述</p></th>
<th class="head"><p>返回值</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TS_MPI_TRP_RNE_LoadModel</p></td>
<td><p>初始化单个NN网络。</p></td>
<td><p>net：网络指针</p></td>
<td><p>0表示成功，非0表示失败</p></td>
</tr>
<tr class="row-odd"><td><p>TS_MPI_TRP_RNE_Forward</p></td>
<td><p>网络前向推理</p></td>
<td><p>net：网络指针</p></td>
<td><p>若返回为NULL，则前向推理出现异常；</p>
<p>若网络的cpDebugLayerName不为空且能找到该调试层，则返回该调试层的输出；</p>
<p>否则返回网络的最后结果，等同于函数TS_MPI_TRP_RNE_GetResultBlobs的返回值</p>
</td>
</tr>
<tr class="row-even"><td><p>TS_MPI_TRP_RNE_UnloadModel</p></td>
<td><p>释放单个网络。</p></td>
<td><p>net：网络指针</p></td>
<td><p>0表示成功，非0表示失败</p></td>
</tr>
<tr class="row-odd"><td><p>TS_MPI_TRP_RNE_RegisterGpUserData</p></td>
<td><p>注册通用算子层自定义数据</p></td>
<td><p>net：网络指针</p></td>
<td><p>0表示成功，非0表示失败</p></td>
</tr>
</tbody>
</table>
<ol class="arabic simple" start="2">
<li><p>执行make命令进行编译、运行</p></li>
</ol>
<p>在simlib目录下执行make指令进行编译，会在 <code class="docutils literal notranslate"><span class="pre">build_sim/Release/RNE_Sim_Lib_demo.elf</span></code> 目录下生成一个仿真elf文件，该文件可以直接在终端执行。</p>
<p>工作目录：<code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/ObjectDetection/csrc/yolov5_simlib</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make<span class="w"> </span><span class="nv">EXPORT_DIR</span><span class="o">=</span>../../../RNESimLibForDemo/RNESimLibD
</pre></div>
</div>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_23.png" />
</figure>
<p></p>
<p>工作目录：/TS-KnightDemo/Resources/ObjectDetection/csrc/yolov5_simlib</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_24.png" />
</figure>
<p></p>
<p>模拟库输出的图片 <code class="docutils literal notranslate"><span class="pre">yolov5_simlib_result.jpg</span></code> 与 <code class="docutils literal notranslate"><span class="pre">TS-RNE-Simulator</span></code>
输出的 <a href="#id47"><span class="problematic" id="id48">``</span></a>bus_sim.jpg``对比，结果一致。</p>
<section id="id49">
<span id="id50"></span><h5>检查点<a class="headerlink" href="#id49" title="Permalink to this heading"></a></h5>
<p>使用模拟器推理完成后，需要进行检查点4与检查点3的数据或者图片的对比，两者结果应一致。</p>
</section>
</section>
</section>
<section id="id51">
<h3>运行时库<a class="headerlink" href="#id51" title="Permalink to this heading"></a></h3>
<section id="id52">
<h4>相关文件说明<a class="headerlink" href="#id52" title="Permalink to this heading"></a></h4>
<p>容器外 <code class="docutils literal notranslate"><span class="pre">/TX5368x_TX5339x_TX5335x_Lib/RNE-RT-Lib/samples</span></code> 目录下有关文件如下：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>文件夹或文件</p></th>
<th class="head"><p>说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>compiled_model</p></td>
<td><dl>
<dt>如果无指定路径，脚本默认此文件夹为模拟器输入文件夹。</dt><dd><ul class="simple">
<li><p>DNN_S_mfcc：根据网络名称创建的文件夹。</p></li>
<li><p>model_r.cfg：网络指令文件。</p></li>
<li><p>model_r.weight：网络权重文件。</p></li>
<li><p>model_input.bin：网络输入文件。</p></li>
</ul>
<p>其他网络。</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>rne_simple_forward</p></td>
<td><p>演示一般网络基本推理过程的示例程序。</p></td>
</tr>
<tr class="row-even"><td><p>rne_set_input_blobs_addr</p></td>
<td><p>此demo演示用户通过物理地址的方式加载输入数据。</p></td>
</tr>
<tr class="row-odd"><td><p>rne_yolov5_detection</p></td>
<td><p>此demo演示yolov5的前处理、推理、后处理的完整流程。</p></td>
</tr>
<tr class="row-even"><td><p>Scripts</p></td>
<td><dl class="simple">
<dt>用例运行脚本及依赖的工具。</dt><dd><ul class="simple">
<li><p>bin2header：二进制文件转头文件脚本。</p></li>
<li><p>run_e2e_example.sh: 总执行入口脚本。</p></li>
<li><p>run_e2e_example_cm.sh: 指定demo和模型路径编译。</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</section>
<section id="demo-2">
<span id="id53"></span><h4>demo运行说明<a class="headerlink" href="#demo-2" title="Permalink to this heading"></a></h4>
<p>以 <code class="docutils literal notranslate"><span class="pre">rne_yolov5_detection</span></code>  为例，执行如下命令：</p>
<p>工作路径： <code class="docutils literal notranslate"><span class="pre">examples/rne_yolov5_detection</span></code></p>
<p>执行命令：<code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">clean</span> <span class="pre">;</span> <span class="pre">make;</span></code></p>
<p>执行成功后，会看到以下信息提示，表示交叉编译成功，生成板端部署资源：<code class="docutils literal notranslate"><span class="pre">examples/rne_yolov5_detection/build_linux_a53/Release/rne_yolov5_detection.elf</span></code>。</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_25.png" />
</figure>
<p></p>
<p>资源成功生成后,如何在板端部署运行，请参见 <a class="reference internal" href="#id54">板端环境搭建及部署</a> 。</p>
</section>
</section>
<section id="id54">
<h3>板端环境搭建及部署<a class="headerlink" href="#id54" title="Permalink to this heading"></a></h3>
<section id="id55">
<h4>环境准备<a class="headerlink" href="#id55" title="Permalink to this heading"></a></h4>
<p>开发板环境的配置及与板端的初始化请参考《TX5368A Linux
SDK安装及升级使用说明_v1.5.pdf》第2章安装、升级TX5368A DEMO板开发环境。</p>
</section>
<section id="id56">
<h4>板端运行<a class="headerlink" href="#id56" title="Permalink to this heading"></a></h4>
<section id="id57">
<h5>配置交叉编译环境<a class="headerlink" href="#id57" title="Permalink to this heading"></a></h5>
<p>在RNE-RT-Lib目录下，有settings_path_linux.sh脚本。编辑该脚本将tools_dir设置成gcc-arm-10.2-2020.11-x86_64-arm-none-linux-gnueabihf.tar.xz
解压后的存放路径（最好是绝对路径），之后source一下就可以。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 工作目录：./RNE_RT_Lib</span>
<span class="c1"># 命令</span>
vi<span class="w"> </span>settings_path_linux.sh
<span class="c1"># 编辑’tools_dir=xxxxxx’</span>

<span class="c1"># 保存退出</span>
:wq

<span class="c1">#source 是使该文件生效</span>
<span class="nb">source</span><span class="w"> </span>settings_path_linux.sh
</pre></div>
</div>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_26.png" />
</figure>
<p></p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_27.png" />
</figure>
<p></p>
</section>
<section id="id58">
<h5>打开串口调试工具<a class="headerlink" href="#id58" title="Permalink to this heading"></a></h5>
<p>确保连线正确，然后打开串口调试工具，可以使用SSCOM、SecureCRT或者其他的软件，本示例使用的是ipop,
确保IPOP工具TFTP已配置好服务器路径，确保打开对应的调试串口，确保板端已经进入linux系统。</p>
</section>
<section id="id59">
<h5>拷贝部署资源到板端部署<a class="headerlink" href="#id59" title="Permalink to this heading"></a></h5>
<p>把rne_yolov5_detection.elf以及examples/rne_yolov5_detection目录下resource文件夹拷贝到window系统下IPOP工具配置TFTP服务器目录下，</p>
<p>然后执行如下命令：通过tftp把文件拷贝到板端：</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>板端的连接配置请参考《TX5368A Linux SDK安装及升级使用说明》。</p>
</div>
<p><strong>部署模型</strong></p>
<dl>
<dt>参数说明：</dt><dd><p>参数1: resource/yolov5_quantize_r.cfg</p>
<p>参数2: resource/yolov5_quantize_r.weight</p>
<p>参数3: resource/1.jpg or resource/2.jpg or resource/3.jpg or resource/4.jpg</p>
<p>参数4: 检测结果图片保存文件名(必须图片格式结尾)</p>
<p>参数5: 前处理是否使用int8 hwc的格式, 0 : 不使用, 1 : 使用./rne_yolov5_detection.elf 参数1 参数2 参数3 参数4 参数5</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<blockquote>
<div><p>读入模型文件cfg和weight时, 需保证*.cfg和*.weight的前缀一致</p>
</div></blockquote>
</div>
<p>执行前的图片为4.jpg</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_28.png" />
</figure>
<p></p>
<p>然后可以看到模型运行结果如下图</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_29.png" />
</figure>
<p></p>
<p>生成的识别后的图片如下：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/demo_30.png" />
</figure>
<p>至此，模型部署完毕！</p>
</section>
</section>
</section>
</section>
<section id="id60">
<h2>自定义算子开发快速指南<a class="headerlink" href="#id60" title="Permalink to this heading"></a></h2>
<section id="id61">
<span id="id62"></span><h3>脚本运行方式<a class="headerlink" href="#id61" title="Permalink to this heading"></a></h3>
<p>在目录 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Scripts</span></code> 中对应的脚本运行如下所示：</p>
</section>
<section id="id63">
<span id="id64"></span><h3>重要步骤<a class="headerlink" href="#id63" title="Permalink to this heading"></a></h3>
<ol class="arabic simple">
<li><p>修改量化后的模型</p></li>
</ol>
<p>将预处理的resize和crop操作作为自定义算子层放在模型中，因此需要在量化后的prototxt模型中增加自定义算子层custom_resize和custom_crop,并修改连接层的bottom值。</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>layer {

name: &quot;custom_resize&quot;

type: &quot;custom_resize&quot;

bottom: &quot;data&quot;

top: &quot;custom_resize&quot;

ts_rce_layer{

layer_type: 1153

top_channel: 3

top_width: 256

top_height: 256

}

fix_param {

input_bit: &quot;us8&quot;

output_bit: &quot;us8&quot;

}

}

layer {

name: &quot;custom_crop&quot;

type: &quot;custom_crop&quot;

bottom: &quot;custom_resize&quot;

top: &quot;custom_crop&quot;

ts_rce_layer{

layer_type: 1154

top_channel: 3

top_width: 224

top_height: 224

}

fix_param {

input_bit: &quot;us8&quot;

output_bit: &quot;us8&quot;

}

}
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>模型编译</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>compile<span class="w"> </span>--net<span class="w"> </span>/TS-KnightDemo/Resources/Classification/resnet18/customized_model/resnet18_quant.prototxt
--weight<span class="w"> </span>/TS-KnightDemo/Resources/Classification/resnet18/customized_model/resnet18_quant.caffemodel
–-save-dir<span class="w"> </span>/TS-KnightDemo/Output/resnet18_customized/rne<span class="w"> </span>-gp<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>自定义算子C语言实现</p></li>
</ol>
<p>首先注册自定义算子层：头文件ts_rne_gp_layers.h中新增如下代码，注意此处和ID和prototxt中层类型layer_type一致。</p>
<blockquote>
<div><figure class="align-center">
<img alt="pipeline" src="../_images/demo_31.png" />
</figure>
</div></blockquote>
<p></p>
<p>文件ts_rne_gp_layers.c中新增如下代码：</p>
<blockquote>
<div><figure class="align-center">
<img alt="pipeline" src="../_images/demo_32.png" />
</figure>
</div></blockquote>
<p>然后C语言实现自定义算子，即编写ts_rne_gp_custom_crop.c和ts_rne_gp_custom_resize.c。</p>
<p>4) 模拟库模拟
切换至自定义算子的源码目录，并将编译后模型使用头文件生成工具做转换</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/TS-KnightDemo/Resources/Classification/csrc/resnet18_customized

./bin2header
/TS-KnightDemo/Output/resnet18_ops_caffe/rne/resnet18_quant_r.cfg
./resnet18_cfg.h

./bin2header
/TS-KnightDemo/Output/resnet18_ops_caffe/rne/resnet18_quant_r.weight
./resnet18_weight.h
</pre></div>
</div>
<p></p>
<p>在 <code class="docutils literal notranslate"><span class="pre">/TS-KnightDemo/Resources/Classification/csrc/resnet18_customized</span></code> 目录下运行make命令，生成elf文件后运行</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make<span class="w"> </span>clean
make
build_sim/Release/RNE_Sim_Lib_demo.elf<span class="w"> </span>/TS-KnightDemo/Output/resnet18_ops_caffe/simlib/
</pre></div>
</div>
<p>运行时库上板执行，参见运行时库的上板说明  <a class="reference internal" href="#id54">板端环境搭建及部署</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright COPYRIGHT© 2024北京清微智能科技有限公司, 保留所有权利。.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>