

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>4. 量化使用指南 &mdash; Knight_doc V3.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=eb26d1a0"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. 编译仿真性能分析使用指南" href="compile.html" />
    <link rel="prev" title="3. 快速上手指南" href="../quick_demo/quick_demo.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Knight_doc
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Knight 工具链</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../doc_info/doc_info.html">1. 修改记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/overview.html">2. 使用指南综述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_demo/quick_demo.html">3. 快速上手指南</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 量化使用指南</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">4.1. 量化介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">4.2. 工具简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">4.3. 程序功能</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">4.4. 参数说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="#onnx">4.5. ONNX模型量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#demo">4.5.1. demo示例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id6">4.5.1.1. 示例一：量化demo模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">4.5.1.2. 示例二：量化时自动在输入后实现减均值除方差操作</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qdq">4.5.1.3. 示例三：量化QDQ模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#iteration">4.5.1.4. 示例四：多量化模式和iteration进行量化</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-config">4.5.1.5. 示例五：–run-config的使用示例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id10">4.5.2. 量化新模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id11">4.5.2.1. ONNX定义前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">4.5.2.2. 注册前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">4.5.2.3. 执行量化命令进行量化</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id14">4.5.3. 混合量化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#json">4.5.3.1. 生成混合量化模板json配置文件</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">4.5.3.2. 更新混合量化json配置文件</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">4.5.3.3. 指定混合量化配置文件开展混合量化</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id17">4.5.3.4. 自动生成混合量化配置文件</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pytorch">4.6. Pytorch模型量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id18">4.6.1. demo量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id20">4.6.2. 量化新模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id21">4.6.2.1. 模型转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">4.6.2.2. 定义前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id23">4.6.2.3. 执行量化命令进行量化</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qat">4.6.3. 量化QAT模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id24">4.6.3.1. 定义前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id25">4.6.3.2. 执行量化命令进行量化</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id26">4.6.4. 约束条件</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#caffe">4.7. Caffe模型量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#caffe-demo">4.7.1. caffe demo量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id27">4.7.2. 量化新模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id28">4.7.2.1. 模型转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id29">4.7.2.2. 定义前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id30">4.7.2.3. 执行量化命令进行量化</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#paddlepaddle">4.8. PaddlePaddle模型量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#demo-2">4.8.1. demo量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id32">4.8.2. 量化新模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id33">4.8.2.1. 模型转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id34">4.8.2.2. 定义前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id35">4.8.2.3. 执行量化命令进行量化</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow">4.9. Tensorflow模型量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id36">4.9.1. demo量化示例</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id37">4.9.2. 量化新模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id38">4.9.2.1. 模型转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id39">4.9.2.2. 定义前向推理函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id40">4.9.2.3. 执行量化命令进行量化</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id41">4.10. 量化后模型路径</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id42">4.11. 测试量化后模型精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id43">4.12. 注意事项</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compile.html">5. 编译仿真性能分析使用指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="sdk.html">6. SDK使用指南</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">进阶指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guides_advanced/qat.html">1. QAT使用说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides_advanced/mc.html">2. 模型压缩使用指南</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/quant_faq.html">1. 量化工具FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op/op.html">2. 算子支持列表</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Knight_doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">4. </span>量化使用指南</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guides_base/quant.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1><span class="section-number">4. </span>量化使用指南<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h1>
<p>本文档主要介绍量化工具使用方法，为产品应用提供技术支持。</p>
<p><strong>名词解释</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>名词</strong></p></th>
<th class="head"><p><strong>说明</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TOOL-ROOT</p></td>
<td><p>量化工具所在的目录</p></td>
</tr>
</tbody>
</table>
<section id="id2">
<h2><span class="section-number">4.1. </span>量化介绍<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<p>量化工具的作用是把用户输入的浮点模型转换成定点模型，将模型转化为全整数计算，从而有效降低了模型的运算量和参数的空间占用量。定点模型以ONNX格式作为IR模型送到编译器进行编译处理。</p>
<p>量化的基本原理是对量化数据集执行浮点模型的前向运算，统计出每层的输入输出和权重数据的分布特征，该特征体现为定点模型文件中的多种scale系数，通过这些系数将浮点数据空间映射到定点数据空间。</p>
<p>量化工具包括Finetune
Lib和ONNX量化工具。其中ONNX量化工具还集成了pytorch2onnx转换模块、caffe2onnx转换模块、tensorflow2onnx转换模块、paddle2onnx转换模块，支持将多种框架下定义的模型转换为ONNX模型，转换后的模型可供ONNX量化工具进一步量化。</p>
<p>不同平台的量化工具所支持的算子稍有差异，具体范围和支持特性需要参考量化算子规格表；不同平台对同一算子的设计原理和实现方案是相同的，但具体实现细节稍有差异，量化精度大体保持一致。为了进一步保证量化精度，降低模型复杂性，量化工具结合芯片规格对部分算子进行了特定优化（比如会将Conv+BN结构或者Conv+BN+Relu结构融合成Conv，其中Conv的权重采用per-channel的方式统计特征信息）。</p>
<p>在下文的描述中，{TOOL-ROOT}在各个章节分别指代对应量化工具的存放目录。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>章节</strong></p></th>
<th class="head"><p><strong>{TOOL-ROOT}的指代路径</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ONNX量化工具的demo</p></td>
<td><p>/opt/Quantize/onnx_examples</p></td>
</tr>
</tbody>
</table>
<p>量化工具支持范围：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>量化支持位宽</p></th>
<th class="head"><p>TX510x</p></th>
<th class="head"><p>TX5336x_TX5256x</p></th>
<th class="head"><p>TX5368x_TX5339x_TX5335x</p></th>
<th class="head"><p>TX5215x_TX5239x200_TX5239x220_TX5239x300</p></th>
<th class="head"><p>TX5112x_TX5239x201</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ONNX量化工具</p></td>
<td><p>8bit</p></td>
<td><p>8/16bit</p></td>
<td><p>8/16bit</p></td>
<td><p>8/16bit</p></td>
<td><p>8/16bit</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id3">
<h2><span class="section-number">4.2. </span>工具简介<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h2>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_1.png" />
</figure>
<p></p>
<p>ONNX量化工具支持对ONNX，Caffe，Pytorch，PaddlePaddle及Tensorflow五种格式浮点模型的量化。其中Caffe，Pytorch，PaddlePaddle和Tensorflow格式的模型需要先转换为ONNX模型后再进行模型量化。ONNX量化工具通过加载用户提供的量化数据集和ONNX浮点模型开展量化，量化完成后生成ONNX格式的IR定点模型提供给下游编译器使用。</p>
<p>/opt/Quantize/onnx_examples目录结构说明如下：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>目录</p></th>
<th class="head"><p>开源/闭源</p></th>
<th class="head"><p>说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>data</p></td>
<td><p>开源</p></td>
<td><p>存放example量化数据。</p></td>
</tr>
<tr class="row-odd"><td><p>infer_auto.py</p></td>
<td><p>开源</p></td>
<td><p>infer_auto函数，自动生成随机化量化数据。</p></td>
</tr>
<tr class="row-even"><td><p>infer_demo.py</p></td>
<td><p>开源</p></td>
<td><p>demo脚本，包括infer函数实现示例，pytorch模型定义示例。</p></td>
</tr>
<tr class="row-odd"><td><p>model/resnet18_caffe/</p></td>
<td><p>开源</p></td>
<td><p>caffe模型，用于caffe2onnx示例。</p></td>
</tr>
<tr class="row-even"><td><p>model/resnet18_mix/</p></td>
<td><p>开源</p></td>
<td><p>混合量化json示例。</p></td>
</tr>
<tr class="row-odd"><td><p>model/resnet18_onnx/</p></td>
<td><p>开源</p></td>
<td><p>onnx模型，用于onnx量化示例等。</p></td>
</tr>
<tr class="row-even"><td><p>model/resnet18_paddle/</p></td>
<td><p>开源</p></td>
<td><p>paddle模型，用于paddle2onnx示例。</p></td>
</tr>
<tr class="row-odd"><td><p>model/resnet18_pytorch/</p></td>
<td><p>开源</p></td>
<td><p>pytorch模型的权重，用于pytorch 2onnx示例。</p></td>
</tr>
<tr class="row-even"><td><p>model/resnet18_qat/</p></td>
<td><p>开源</p></td>
<td><p>qat示例。</p></td>
</tr>
<tr class="row-odd"><td><p>model/resnet18_tf/</p></td>
<td><p>开源</p></td>
<td><p>tf模型，用于tf2onnx示例。</p></td>
</tr>
<tr class="row-even"><td><p>model/resnet18_qdq/</p></td>
<td></td>
<td><p>qdq示例。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id4">
<h2><span class="section-number">4.3. </span>程序功能<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h2>
<p>ONNX量化工具根据提供的量化数据集和浮点模型（支持ONNX, Caffe, Pytorch, PaddlePaddle, Tensorflow格式）进行量化，量化完成后生成ONNX格式的IR定点模型提供给下游编译器使用。</p>
</section>
<section id="id5">
<h2><span class="section-number">4.4. </span>参数说明<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h2>
<dl>
<dt>-h或–help</dt><dd><ul class="simple">
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：显示帮助信息。</p></li>
</ul>
</dd>
<dt>-m或–model</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>必选</p></li>
<li><p>默认值：无</p></li>
<li><dl class="simple">
<dt>说明：</dt><dd><ul>
<li><p>对ONNX格式：指ONNX模型文件的路径，支持相对路径和绝对路径。</p></li>
<li><p>对Pytorch格式：指模型名称，需在-uds参数指定的python脚本中定义一个同名函数，函数返回两个值，分别是模型定义和输入tensor。</p></li>
<li><p>对Paddle格式：指pdmodel模型文件的路径，支持相对路径和绝对路径。</p></li>
<li><p>对Caffe格式：指prototxt模型文件的路径，支持相对路径和绝对路径。</p></li>
<li><p>对Tensorflow格式：指Tensorflow模型文件的路径，支持相对路径和绝对路径。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>-f或–framework</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：onnx</p></li>
<li><p>说明：待量化模型所属框架类型：若为onnx模型，则直接量化，其它四种模型格式则需要转换再进行量化。取值范围：[onnx, pytorch, caffe,paddle, tensorflow]</p></li>
</ul>
</dd>
<dt>-if或–infer-func</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：infer_auto</p></li>
<li><p>说明：前向推理函数名称。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>infer_auto为量化工具内置的自动根据输入shape生成随机数据开展量化的infer函数，具体说明可参见FAQ文档。</p>
</div>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--save-dir</span></kbd></dt>
<dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：/TS-KnightOutput/QuantOnnx/</p></li>
<li><p>说明：指定量化模型的保存路径。指定–save-dir时，量化模型保存{–save-dir}/下。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>-d或–data</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：量化输入数据路径，支持相对路径和绝对路径。</p></li>
</ul>
</dd>
<dt>-gsj或–generate-scale-json</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: action</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：若设置，量化过程中会生成存有量化scale信息的json文件，保存在{–save-dir}/目录下。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意：run-mode为quant时才生效，且该参数与-lsj互斥。</p>
</div>
</dd>
<dt>-lsj或–load-scale-json</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：存放量化scale信息的json文件路径，支持相对路径和绝对路径。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>该参数与-gsj/-mc互斥。</p>
</div>
</dd>
<dt>-b或–bit-width</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：8</p></li>
<li><p>说明：设置模型用8bit量化还是16bit量化。当设置16时，需要确保配套硬件是支持16bit量化的，否则量化出的模型无法在硬件上运行。</p></li>
</ul>
</dd>
<dt>-i或–iteration</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：1</p></li>
<li><p>说明：量化时模型执行推理的次数。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>需要选取合适的样本数量才能达到理想的量化模型精度。支持传入多值进行不同数据量化，多值输入时使用空格分割例如-i 1 10 此参数多值输入会分别按照每个值进行量化。</p>
</div>
</dd>
<dt>-bs或–batch-size</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：1</p></li>
<li><p>说明：量化模型时加载量化数据的batchsize大小，量化过程实际用到的数据量是 <code class="docutils literal notranslate"><span class="pre">iteration*</span> <span class="pre">batch-size</span></code> 。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1）此参数仅在模型为动态图时可配置，且需要用户在infer函数里实现相应逻辑才能生效。</p>
<p>2）量化后模型不支持动态图。若原始浮点模型为动态batch，则需设置此参数为与量化时数据相同的batch-size值，否则可能导致量化异常。</p>
</div>
</dd>
<dt>-ib或–ir-batch</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：1</p></li>
<li><p>说明：设置量化后模型的batch-size。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>仅当原始浮点模型为动态batch时此参数方生效，若为固定输入模型此参数不生效。不支持多输入为动态batch且量化后对应输入的batch不一致的场景。</p>
</div>
</dd>
<dt>-du或–dump</dt><dd><ul class="simple">
<li><p>参数类型: action</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：若设置，量化过程中会保存每个算子的输出到本地文件，供后面精度对比工具使用。数据结果保存在{–save-dir}/目录下。</p></li>
</ul>
</dd>
<dt>-gt或–generate-template</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: action</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：若设置，会生成模型对应的混合量化模板json配置文件，文件保存在{–save-dir}/目录下。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>run-mode为quant时才生效</p>
</div>
</dd>
<dt>-mc或–mix-config</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：混合量化json文件路径，支持相对路径和绝对路径，需要确保配套硬件是支持混合量化。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>该参数与-lsj互斥。</p>
</div>
</dd>
<dt>-l或–log-level</dt><dd><ul>
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：3</p></li>
<li><p>说明：指定量化工具日志级别：</p>
<blockquote>
<div><ul class="simple">
<li><p>0:  显示DEBUG日志。</p></li>
<li><p>1:  显示INFO日志。</p></li>
<li><p>2:  显示WARNING日志。</p></li>
<li><p>3:  显示ERROR日志。</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
<dt>-qm或–quant-mode</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：8bit量化时，默认值为kl;16bit量化时，默认值是min_max</p></li>
<li><dl class="simple">
<dt>说明：计算激活值的scale系数采用的方法，支持模式如下</dt><dd><ul>
<li><p>kl： kl散度</p></li>
<li><p>min_max：最大最小值</p></li>
<li><p>mse：均方误差</p></li>
<li><p>percentile：保留指定百分比数据 kl_v2：修正特定情况的kl散度</p></li>
<li><p>all：分别执行上述所有模式进行量化</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>支持传入多值进行不同数据量化，多值输入时使用空格分割例如-qm kl mse</p></li>
<li><p>此参数多值输入会分别按照每个值进行量化。</p></li>
</ol>
</div>
</dd>
<dt>-per或–percent</dt><dd><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>可选</p></li>
<li><p>默认值：0.99999</p></li>
<li><p>说明：仅在quant-mode设置为percentile时生效，设定量化百分位，保留其指定的数据量，例如值为0.9999时当统计数据量达到10000时丢弃一个数值。建议数值：[0.9999，0.99999，0.999999]，不建议设置太小丢弃太多数值可能会导致精度大幅下降。</p></li>
</ul>
</dd>
<dt>-r或–run-mode</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：quant</p></li>
<li><dl class="simple">
<dt>说明：量化模式选择：取值范围[quant,infer,convert, compare]</dt><dd><ul>
<li><p>quant：量化模式，对浮点模型做量化和定点。</p></li>
<li><p>infer：推理模式，可使用浮点或量化后模型进行推理。</p></li>
<li><p>convert：转换模式，只进行模型转换不进行模型量化。</p></li>
<li><p>compare: 数据对比模式，功能上等同于-du。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>-ll或–lut-len</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：10</p></li>
<li><p>说明：指定LUT表的长度，取值范围[8,9,10,11,12].</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>此参数仅在16bit量化时才生效</p>
</div>
</dd>
<dt>-lt或–layer-threshold</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：指定LayerNormalization的输入输出范围。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意： 此参数仅在LayerNormalization层精度骤降时使用</p>
</div>
</dd>
<dt>-qid或–quantize-input-dtype</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：None</p></li>
<li><p>说明：指定量化后模型输入数据类型：取值范围：[float32, int8,uint8, int16, None]</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1)若某输入的下一个量化节点为8bit，则此输入不可指定为int16。</p>
<p>2)可传入一个值或者与输入数量相同的值，传入一个值时会自动广播为与输入数量相同的值。</p>
<p>3)仅当原始浮点模型的输入为float32时才可使用此参数进行输入类型的更改。</p>
<p>4)None表示保持原有输入类型不变。</p>
<p>5)量化后模型推理时需要指定与量化时相同的qid值。</p>
</div>
</dd>
<dt>-od或–output-dequant</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: action</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：是否增加反量化，若设置，会在所有输出层算子前增加反量化算子。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>若某输出层的上方算子为ArgMax或者软件层，则对此输出层不生效不插入反量化算子。</p>
</div>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--mean</span></kbd></dt>
<dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：指定输入后需要增加的BN算子的均值，在输入后需要增加减均值除方差操作时使用。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1)run-mode为quant时才生效。</p>
<p>2)需和–std同时使用。</p>
<p>3)mean的个数必须等于输入节点的channel(对应为各通道的均值)或者一个值(所有通道的均值)。</p>
</div>
</dd>
<dt><kbd><span class="option">--std</span></kbd></dt>
<dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：指定输入后需要增加的BN算子的方差，在输入后需要增加减均值除方差操作时使用。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1)run-mode为quant时才生效。</p>
<p>2)需和–mean同时使用。</p>
<p>3)std的个数必须等于输入节点的channel(对应为各通道的方差)或者一个值(所有通道的方差)。</p>
<p>4)仅支持一组mean/std，对于多输入网络仅作用于第一路输入。</p>
</div>
</dd>
</dl>
<dl>
<dt>-w或–weight</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><dl class="simple">
<dt>说明：</dt><dd><ul>
<li><p>对Pytorch格式：指pth权重文件的路径，支持相对路径和绝对路径。</p></li>
<li><p>对Caffe格式：指caffemodel权重文件的路径，支持相对路径和绝对路径。</p></li>
<li><p>对Paddle格式：指pdiparams权重文件的路径，支持相对路径和绝对路径。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>-snn或–start-node-names</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>必选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：指定Tensorflow模型量化开始节点名，仅在Tensorflow模型量化过程中使用。多输入使用空格分割，例如 -snn input1 input2。</p></li>
</ul>
</dd>
<dt>-enn或–end-node-names</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>必选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：指定Tensorflow模型量化结束节点名，仅在Tensorflow模型量化过程中使用。多输出使用空格分割，例如 -enn output1 output2。</p></li>
</ul>
</dd>
<dt>-c2chw或–convert2chw</dt><dd><blockquote>
<div><ul class="simple">
<li><p>参数类型: action</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：Tensorflow模型输入的format为4维NHWC，可通过指定该参数使转出的onnx模型从输入开始的format都为NCHW。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>适用场景：</dt><dd><p>1) Tensorflow 4维模型
2)只对模型第一层算子为Conv、AveragePool、GlobalAveragePool、MaxPool、GlobalMaxPool、Resize等有效；</p>
</dd>
</dl>
</li>
<li><p>推理时用户需要输入数据的format转换为NCHW。</p></li>
</ol>
</div>
</dd>
<dt>-is或–input-shapes</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：输入数据的shape，例如Imagenet数据集的输入：1 3 224 224，量化参数设置为-is 1 3 224 224；当模型为多输入时，需按顺序分别指定各个输入的shape，例如双输入模型，浮点输入分别为1 3 224 224、 1 3 224 224，则设置为-is 1 3 224 224 -is 1 3 224 224, 仅在Paddle模型量化过程中使用。</p></li>
</ul>
</dd>
<dt>-uds或–user-defined-script</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：指定用户自定义的python脚本，用于加载推理函数、加载pytorch模型定义。</p></li>
</ul>
</dd>
<dt>-cn或–cpu-num</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：5</p></li>
<li><p>说明：设置量化并行计算的CPU数目。</p></li>
</ul>
</dd>
<dt>-cd或–cache-distribution</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：scale统计直方图缓存文件路径，设置该参数，则会加载缓存文件，跳过scale计算前向推理过程。如果该路径或路径下缓存文件不存在，则会在第一次设置该参数时生成缓存文件。若量化数据改变，则需要重新生成缓存文件。</p></li>
</ul>
</dd>
<dt>-uis或–unify-input-scale</dt><dd><ul class="simple">
<li><p>参数类型: action</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：是否对Concat，Stack和ScatterND类型的算子进行系数统一。默认关闭，若设置则将以上所述类别算子的输入系数统一为相同值。</p></li>
</ul>
</dd>
<dt>-amr 或–auto-mix-ratio</dt><dd><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>可选</p></li>
<li><p>默认值：0.5</p></li>
<li><p>说明：生成混合量化模板时，低比特比率。</p></li>
</ul>
</dd>
<dt>-ams 或–auto-mix-strategy</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：initial</p></li>
<li><dl class="simple">
<dt>说明：指定混合量化模板生成策略。支持[‘HAWQ’, ‘IOhigh’, ‘initial’]。</dt><dd><ul>
<li><p>‘HAWQ’：根据神经网络损失函数的二阶海森矩阵配置比特位宽。</p></li>
<li><p>‘IOhigh’：对输入、输出的多个层配置为高比特位宽，中间层配置为低比特位宽。</p></li>
<li><p>‘initial’: 默认策略，根据命令行入参–bit-width和–quant-mode生成相应配置。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>-rc或–run-config</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：包含量化相关的参数配置。</p></li>
</ul>
</dd>
<dt>-on 或 –output-name</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>说明：量化输出节点裁剪。设置输出节点名，会按输出名进行裁剪和量化。</p></li>
</ul>
</dd>
</dl>
</section>
<section id="onnx">
<h2><span class="section-number">4.5. </span>ONNX模型量化<a class="headerlink" href="#onnx" title="Permalink to this heading"></a></h2>
<p>ONNX模型量化主体流程如下图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_2.png" />
</figure>
<p></p>
<section id="demo">
<h3><span class="section-number">4.5.1. </span>demo示例<a class="headerlink" href="#demo" title="Permalink to this heading"></a></h3>
<section id="id6">
<h4><span class="section-number">4.5.1.1. </span>示例一：量化demo模型<a class="headerlink" href="#id6" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-qm<span class="w"> </span>kl<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示模型量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_3.png" />
</figure>
<p></p>
</section>
<section id="id7">
<h4><span class="section-number">4.5.1.2. </span>示例二：量化时自动在输入后实现减均值除方差操作<a class="headerlink" href="#id7" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18<span class="w"> </span>-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-qm<span class="w"> </span>kl<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-uds
/opt/Quantize/onnx_examples/infer_demo.py<span class="w"> </span>--std<span class="w"> </span><span class="m">1</span>.2<span class="w"> </span><span class="m">1</span>.1<span class="w"> </span><span class="m">1</span>.3<span class="w"> </span>--mean<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="m">0</span>.2<span class="w"> </span><span class="m">0</span>.3
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示模型量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_4.png" />
</figure>
<p></p>
</section>
<section id="qdq">
<h4><span class="section-number">4.5.1.3. </span>示例三：量化QDQ模型<a class="headerlink" href="#qdq" title="Permalink to this heading"></a></h4>
<p>ONNX量化工具可自动识别QDQ格式的模型并加载其中的系数开展后续量化，量化流程与普通浮点模型量化一致。示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_qdq/resnet18_qdq.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-qm<span class="w"> </span>kl<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示模型量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_5.png" />
</figure>
<p></p>
</section>
<section id="iteration">
<h4><span class="section-number">4.5.1.4. </span>示例四：多量化模式和iteration进行量化<a class="headerlink" href="#iteration" title="Permalink to this heading"></a></h4>
<p>ONNX量化工具支持同时指定多个quant-mode和iteration进行量化，自动识别和组合两者进行量化，并根据Quantization
Loss进行升序排列，多策略量化时只显示每种策略的开始和结束，示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-qm<span class="w"> </span>kl<span class="w"> </span>min_max<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">10</span>
-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示量化成功，Quantization Loss显示Fail表示当前策略量化失败：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_6.png" />
</figure>
<p></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>多模式量化执行默认会开启数据缓存，若-s目录下存在数据缓存文件需确认缓存的数据量是否与当前量化对应的数据量一致，推荐多模式量化时-s指定为空的新目录。</p>
</div>
</section>
<section id="run-config">
<h4><span class="section-number">4.5.1.5. </span>示例五：–run-config的使用示例<a class="headerlink" href="#run-config" title="Permalink to this heading"></a></h4>
<p>input-configs参数说明：参数的必需/可选仅针对其生效任务；input-configs中所有参数仅在指定uds为infer_common.py以及infer_func为infer_default时生效。</p>
<dl class="py data">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">数据相关参数</span></span><a class="headerlink" href="#id0" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>data_dir</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>必选</p></li>
<li><p>默认值：无</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：图片存储目录。图片格式要求为bmp, jpg, jpeg, png。</p></li>
</ul>
</dd>
<dt>label_dir</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>必选</p></li>
<li><p>默认值：无</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：标签存储目录。检测任务中支持yolo与coco格式标签；分类任务中标签文件为labels.txt，包含两列数据，第一列为图片名，第二列为图片类别号。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="id8">
<span class="sig-name descname"><span class="pre">预处理相关参数</span></span><a class="headerlink" href="#id8" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>is_yolo</dt><dd><ul class="simple">
<li><p>参数类型: bool</p></li>
<li><p>必选</p></li>
<li><p>默认值：false</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：是否进行检测任务相关的预处理，检测任务中为true，分类任务中为false</p></li>
</ul>
</dd>
<dt>color_space</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>必选</p></li>
<li><p>默认值：BGR</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：图片的目标色彩通道，可选项为RGB, BGR, GRAY</p></li>
</ul>
</dd>
<dt>augment</dt><dd><ul class="simple">
<li><p>参数类型: bool</p></li>
<li><p>必选</p></li>
<li><p>默认值：false</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：检测任务中是否对小型图片进行扩大，需要扩大为true，不需要为false</p></li>
</ul>
</dd>
<dt>mean</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>可选</p></li>
<li><p>默认值：无</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：均值，以list形式给出，必须与通道数量相同，与std同时提供时生效</p></li>
</ul>
</dd>
<dt>std</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>必选</p></li>
<li><p>默认值：无</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：标准差，以list形式给出，必须与通道数量相同，与mean同时提供时生效</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="id9">
<span class="sig-name descname"><span class="pre">后处理相关参数</span></span><a class="headerlink" href="#id9" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>calculate_acc</dt><dd><ul class="simple">
<li><p>参数类型: bool</p></li>
<li><p>必选</p></li>
<li><p>默认值：false</p></li>
<li><p>生效任务：检测、分类</p></li>
<li><p>说明：是否计算精度，需要提供标签数据，当未提供label_dir时或label_dir为空时会将浮点模型的输出作为真实标签，并生成标签数据，
存储位置为data_dir同级目录下的labels文件夹下或label_dir指定的目录</p></li>
</ul>
</dd>
<dt>draw_box</dt><dd><ul class="simple">
<li><p>参数类型: bool</p></li>
<li><p>必选</p></li>
<li><p>默认值：false</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务中是否输出画框图片，该选项仅在infer模式下生效，画框图片存储在data_dir同级目录下的label_imgs文件夹下</p></li>
</ul>
</dd>
<dt>max_det</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>必选</p></li>
<li><p>默认值：300</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务中的最大检测数量</p></li>
</ul>
</dd>
<dt>anchors</dt><dd><ul class="simple">
<li><p>必选</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务的锚框，提供该选项时以list形式给出，以像素为单位</p></li>
</ul>
</dd>
<dt>nc</dt><dd><ul class="simple">
<li><p>参数类型: int</p></li>
<li><p>必选</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务的类别数量</p></li>
</ul>
</dd>
<dt>conf_thres</dt><dd><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>必选</p></li>
<li><p>默认值：0.001</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务中nms的置信度阈值</p></li>
</ul>
</dd>
<dt>iou_thres</dt><dd><ul class="simple">
<li><p>参数类型: float</p></li>
<li><p>必选</p></li>
<li><p>默认值：0.6</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务中nms的交并比阈值</p></li>
</ul>
</dd>
<dt>names</dt><dd><ul class="simple">
<li><p>参数类型: string</p></li>
<li><p>可选</p></li>
<li><p>生效任务：检测</p></li>
<li><p>说明：检测任务中各类别对应名称，以list形式给出，提供时会在框体显示类别名称</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<ol class="arabic simple">
<li><p>分类任务示例</p></li>
</ol>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_7.png" />
</figure>
<p></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>quant<span class="w"> </span>--run-config<span class="w"> </span>/opt/Quantize/onnx_examples/config/run_config_cls.json<span class="w">                                                  </span><span class="p">|</span>
</pre></div>
</div>
<p></p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_8.png" />
</figure>
<p></p>
<ol class="arabic simple" start="2">
<li><p>检测任务示例</p></li>
</ol>
<p>量化示例</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_9.png" />
</figure>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>quant<span class="w"> </span>--run-config<span class="w"> </span>/opt/Quantize/onnx_examples/config/run_config_detect.json
</pre></div>
</div>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_10.png" />
</figure>
<p></p>
<p>画框示例</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_11.png" />
</figure>
<p></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>quant<span class="w"> </span>--run-config<span class="w"> </span>/opt/Quantize/onnx_examples/config/run_config_detect.json
</pre></div>
</div>
<p></p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_12.png" />
</figure>
<p></p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_13.png" />
</figure>
</section>
</section>
<section id="id10">
<h3><span class="section-number">4.5.2. </span>量化新模型<a class="headerlink" href="#id10" title="Permalink to this heading"></a></h3>
<section id="id11">
<h4><span class="section-number">4.5.2.1. </span>ONNX定义前向推理函数<a class="headerlink" href="#id11" title="Permalink to this heading"></a></h4>
<p>量化过程中，需要前向推理函数加载数据并运行，通过数据统计来量化模型。
推理函数中，需要用户完成：</p>
<ol class="arabic simple">
<li><p>完成校准数据集的加载及数据预处理。</p></li>
<li><p>执行模型的前向推理。</p></li>
<li><p>打印每个迭代的精度指标和最后总的精度指标。</p></li>
</ol>
</section>
<section id="id12">
<h4><span class="section-number">4.5.2.2. </span>注册前向推理函数<a class="headerlink" href="#id12" title="Permalink to this heading"></a></h4>
<p>1.
指定用户自定义脚本。使用命令行参数 <code class="docutils literal notranslate"><span class="pre">--user-defined-script</span></code> 指定用户自定义脚本的路径，并在该脚本中实现前向推理函数。</p>
<p>2.
注册前向推理函数。在自定义脚本中导入注册函数 <code class="docutils literal notranslate"><span class="pre">onnx_infer_func</span></code>，使用装饰器装饰推理函数，装饰器参数与推理函数名一致。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">onnx_quantize_tool.common.register</span> <span class="kn">import</span> <span class="n">onnx_infer_func</span> <span class="c1">#导入注册函数</span>

<span class="o">@</span> <span class="n">onnx_infer_func</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;infer_cls_model&quot;</span><span class="p">)</span> <span class="c1">#使用装饰器装饰推理函数</span>
<span class="k">def</span> <span class="nf">infer_cls_model</span><span class="p">(</span><span class="n">executor</span><span class="p">):</span>
    <span class="c1"># 获取batch_size</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="c1"># Setting True to enable the early stop for fix forward (可选)</span>
    <span class="c1"># 1：校准数据加载及数据预处理</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="s1">&#39;data/imagenet_input_b10_s1.npy&#39;</span>
    <span class="n">test_label</span> <span class="o">=</span> <span class="s1">&#39;data/imagenet_label_b10_s1.npy&#39;</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">label_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">test_label</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">label_data</span><span class="p">)</span>
    <span class="n">total_correct_num</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_num</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_loader</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="c1"># 2：执行模型前向推理</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="c1"># 3：后处理部分，根据输出结果计算得到对应精度指标</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">correct_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">num</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">total_correct_num</span> <span class="o">+=</span> <span class="n">correct_num</span>
        <span class="n">total_num</span> <span class="o">+=</span> <span class="n">num</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="mf">100.0</span> <span class="o">*</span> <span class="n">correct_num</span><span class="p">)</span> <span class="o">/</span> <span class="n">num</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;acc:&quot;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">executor</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">executor</span><span class="o">.</span><span class="n">early_stop_flag</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span> <span class="c1"># Turning True when the early stop triggered</span>
            <span class="k">break</span>
    <span class="n">total_acc</span> <span class="o">=</span> <span class="p">(</span><span class="mf">100.0</span> <span class="o">*</span> <span class="n">total_correct_num</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_num</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;total_acc:&quot;</span><span class="p">,</span> <span class="n">total_acc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_acc</span>
</pre></div>
</div>
<p><strong>注意：</strong></p>
<ol class="arabic simple">
<li><p>函数入参固定为（executor）。其中executor为量化信息载体实例，其中包含量化信息、量化编解码器等。前向推理函数中可能用到的参数及含义说明见下表：</p></li>
</ol>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>参数名称</strong></p></th>
<th class="head"><p><strong>参数含义</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>executor.iteration</p></td>
<td><p>命令行传递的iteration参数</p></td>
</tr>
<tr class="row-odd"><td><p>executor.batch_size</p></td>
<td><p>命令行传递的batch-size参数</p></td>
</tr>
<tr class="row-even"><td><p>executor.dataset</p></td>
<td><p>命令行传递的data参数</p></td>
</tr>
<tr class="row-odd"><td><p>executor.input_nodes</p></td>
<td><p>模型输入名称列表</p></td>
</tr>
<tr class="row-even"><td><p>executor.output_name_to_node_name()</p></td>
<td><p>所有算子的输出名与算子名称的字典</p></td>
</tr>
<tr class="row-odd"><td><p>executor.shape_dicts</p></td>
<td><p>infer函数中执行exe
cutor.init_shape_info()，可获得层
名与对应shape的字典。详见FAQ文档</p></td>
</tr>
<tr class="row-even"><td><p>executor.early_stop_flag</p></td>
<td><p>early stop标志位</p></td>
</tr>
</tbody>
</table>
<p>用户可利用迭代次数”iteration”参数控制推理的迭代次数，并实现相应的退出逻辑。可利用 <code class="docutils literal notranslate"><span class="pre">dataset</span></code> 和 <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> 参数完成输入数据的正确加载。</p>
<ol class="arabic simple">
<li><p>函数出参固定为（模型精度指标）。模型精度指标一般说来是指模型的评价指标，比如准确率，如果没有这样的指标，建议返回1。</p></li>
<li><p>前向推理函数中第二部分（执行模型前向推理）格式固定，第一部分（数据预处理）和第三部分（数据后处理）需要用户根据实际模型进行相应实现。</p></li>
<li><p>模型前向推理固定为executor.forward(data_1,data_2,…,data_n)，只需按照模型各输入节点顺序依次送入对应的输入数据即可，输入数据data_n必须为numpy数据格式且与模型定义中对应的输入的shape一致，原始数据可为任意数据类型并非为npy格式文件，图片等类型数据格式可参照FAQ文档中ONNX相关章节进行实现。返回结果为list类型，包含了模型各输出节点的输出结果且与各输出节点顺序一致。</p></li>
<li><p>使用early
stop减少量化定点的迭代次数，提高量化速度。使用方法：在推理函数中，通过检查executor.early_stop_flag
== True，捕获循环break信号。</p></li>
</ol>
</section>
<section id="id13">
<h4><span class="section-number">4.5.2.3. </span>执行量化命令进行量化<a class="headerlink" href="#id13" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-qm<span class="w"> </span>kl<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
</section>
</section>
<section id="id14">
<h3><span class="section-number">4.5.3. </span>混合量化<a class="headerlink" href="#id14" title="Permalink to this heading"></a></h3>
<p>ONNX量化工具支持算子层级粒度的8与16bit混合及量化模式min_max/kl/kl_v2/percentile/mse混合，具体混合方式通过json文件的方式进行配置，配置完成后在量化时通过-mc指定此配置文件即可进行混合量化。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>仅TX5368x_TX5339x_TX5335x、TX5215x_TX5239x200_TX5239x220_TX5239x300、TX5336x_TX5256x、TX5112x_TX5239x201支持混合量化。</p>
</div>
<section id="json">
<h4><span class="section-number">4.5.3.1. </span>生成混合量化模板json配置文件<a class="headerlink" href="#json" title="Permalink to this heading"></a></h4>
<p>使用ONNX量化工具可生成模型对应的混合量化模板json配置文件，命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx<span class="w"> </span>-gt
</pre></div>
</div>
<p>上述命令执行后，屏幕打印如下信息则表示成功生成模板json文件：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_14.png" />
</figure>
<p></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>json配置文件只包含量化算子的量化策略信息，非量化算子(如Reshape)无需配置量化策略，也不会出现在配置文件中。</p></li>
<li><p>输入模型先进行图优化进而生成相对应的配置文件，可能出现部分被图优化策略优化或者融合掉的算子，此类算子不再出现在json配置文件中。</p></li>
</ol>
</div>
</section>
<section id="id15">
<h4><span class="section-number">4.5.3.2. </span>更新混合量化json配置文件<a class="headerlink" href="#id15" title="Permalink to this heading"></a></h4>
<p>按照既定混合量化策略修改json配置文件即可。</p>
<p>更新配置文件时需注意以下事项：</p>
<ol class="arabic simple">
<li><p>模板json配置文件中的key值均与待量化算子名称一致，不可更改。</p></li>
<li><p>每个量化节点均必须设置bitwidth与mode, 两者缺一不可。</p></li>
<li><p>bitwidth只支持设置为8或者16。</p></li>
<li><p>mode支持设置为min_max/kl/kl_v2/percentile/mse。</p></li>
<li><p>循环类算子(RNN/LSTM/GRU/SRU/LSTMP)和LayerNormalization类型算子的前后量化算子的量化位宽即bitwidth必须与其一致。</p></li>
<li><p>Abs算子目前不支持混合量化。</p></li>
<li><p>多输入时各输入的量化位宽必须一致。</p></li>
<li><p>Concat算子的父量化节点若存在循环类算子或LayerNormalization类型算子时，
其量化位宽必须不高于其余父子量化节点的量化位宽。</p></li>
</ol>
</section>
<section id="id16">
<h4><span class="section-number">4.5.3.3. </span>指定混合量化配置文件开展混合量化<a class="headerlink" href="#id16" title="Permalink to this heading"></a></h4>
<p>量化时通过-mc指定混合量化json配置文件即可开展混合量化，命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-mc<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_mix/resnet18_mix.json
-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>上述命令执行后，屏幕打印如下信息则表示模型开展混合量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_15.png" />
</figure>
<p></p>
</section>
<section id="id17">
<h4><span class="section-number">4.5.3.4. </span>自动生成混合量化配置文件<a class="headerlink" href="#id17" title="Permalink to this heading"></a></h4>
<p>在生成混合量化模板json配置文件时，可加上–auto-mix-ratio、
–auto-mix-strategy命令行参数，自动生成性能较优的混合量化模板。命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>/tmp/resnet18
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_onnx/resnet18.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-mc<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_mix/resnet18_mix.json
-r<span class="w"> </span>quant<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
--auto-mix-ratio<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span>--auto-mix-strategy<span class="w"> </span>IOhigh<span class="w"> </span>-gt
</pre></div>
</div>
</section>
</section>
</section>
<section id="pytorch">
<h2><span class="section-number">4.6. </span>Pytorch模型量化<a class="headerlink" href="#pytorch" title="Permalink to this heading"></a></h2>
<p>Pytorch模型量化主体流程如下图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_16.png" />
</figure>
<p></p>
<section id="id18">
<h3><span class="section-number">4.6.1. </span>demo量化<a class="headerlink" href="#id18" title="Permalink to this heading"></a></h3>
<p>示例一，量化常规Pytorch模型，命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>–m<span class="w"> </span>resnet18<span class="w"> </span>-f<span class="w"> </span>pytorch
--save-dir<span class="w"> </span>/tmp/result<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_pytorch/resnet18-5c106cde.pth
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>ONNX量化工具内部会将Pytorch模型先转换为ONNX模型再执行量化流程。上述命令执行后，屏幕打印如下信息则表示模型量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_17.png" />
</figure>
<p></p>
<p>示例二：量化QAT模型</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_qdq/resnet18_qdq.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>--save-dir<span class="w"> </span>result<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>需配套提供由清微Finetune-Lib库训练产生qdq模型文件，执行量化流程，屏幕打印如下信息则表示模型量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_18.png" />
</figure>
<p id="id19"></p>
</section>
<section id="id20">
<h3><span class="section-number">4.6.2. </span>量化新模型<a class="headerlink" href="#id20" title="Permalink to this heading"></a></h3>
<section id="id21">
<h4><span class="section-number">4.6.2.1. </span>模型转换<a class="headerlink" href="#id21" title="Permalink to this heading"></a></h4>
<p>Pytorch模型转换有两种方式：1）使用ONNX量化工具自带的转换工具pytorch2onnx；2）Pytorch官方转换接口torch.onnx.export。以下就两种转换方式详细说明：</p>
<ol class="arabic simple">
<li><p><strong>pytorch2onnx转换</strong></p></li>
</ol>
<p>在对Pytorch模型量化之前，量化工具内部需先进行模型转换即将Pytorch模型转换为ONNX模型（组成Pytorch模型的算子集合须符合 <a class="reference internal" href="../op/op.html"><span class="doc">算子支持列表</span></a> ，转换成功后，再对转换出的ONNX模型进行量化。</p>
<p>使用命令行参数–user-defined-script指定用户自定义脚本的路径，并在该脚本中实现Pytorch模型定义函数。程序会将该脚本所在文件夹自动添加到环境变量${PYTHONPATH}中，若模型定义需要添加其他环境变量，需要在执行程序前，手动在docker镜像中设置环境变量。</p>
<p>在自定义脚本中导入Pytorch模型注册函数pytorch_model。使用装饰器装饰模型定义函数，装饰器参数与函数名一致。</p>
<p>转换模型时，使用命令行参–model指定Pytorch模型定义函数名，使用–user-defined-script指定用户自定义脚本，使用–weight指定Pytorch模型权重。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">onnx_quantize_tool.common.register</span> <span class="kn">import</span> <span class="n">pytorch_model</span>  <span class="c1"># 导入Pytorch模型注册函数</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="nd">@pytorch_model</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;resnet18&quot;</span><span class="p">)</span>  <span class="c1"># 使用装饰器装饰模型定义函数</span>
<span class="k">def</span> <span class="nf">resnet18</span><span class="p">(</span><span class="n">weight_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># 函数名必须与args.model一致</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span> <span class="c1"># 模型定义，模型中所有算子须遵从 :doc:`算子支持列表&lt;../op/op&gt;`  之规格。</span>
    <span class="k">if</span> <span class="n">weight_path</span><span class="p">:</span><span class="c1"># 模型加载</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weight_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span><span class="n">model</span><span class="p">,</span><span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">)]}</span> <span class="c1"># 按字典形式返回加载权重后的模型和转换后模型的输入tensor，key值须为“model”和“inputs”。</span>
</pre></div>
</div>
<p>上面示例展示了resnet18模型的函数定义，用户需在该函数实现model权重加载和实例化输入tensor以供程序内部调用和解析。
用户可根据模型的输入个数和类型自行实例化输入tensor，如 <code class="docutils literal notranslate"><span class="pre">[torch.randn(10,3,224,224)</span></code> , <code class="docutils literal notranslate"><span class="pre">torch.randn(10,3,224,224).to(torch.int64)]</span></code>。</p>
<p>若用户需要更改转换后模型输入数据的batchsize（只支持N维为动态 batch，CHW轴不支持动态配置），用户需在返回字典加入
dynamic_axes_inputs:{‘input’:{0: ‘batch_size’}} 和 “dynamic_axes_outputs”: {‘output’:{0:’batch_size’}}，</p>
<p>其中 <code class="docutils literal notranslate"><span class="pre">dynamic_axes_inputs</span></code> 和 <code class="docutils literal notranslate"><span class="pre">dynamic_axes_outputs</span></code> 为固定键名，以 <code class="docutils literal notranslate"><span class="pre">input</span></code> 和 <code class="docutils literal notranslate"><span class="pre">output</span></code> 分别指定要动态batch的输入输出名，
输入输出名可任意指定，0表示第0维（一般为batch维度）为动态维度， <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> 代指动态维度的  <code class="docutils literal notranslate"><span class="pre">shape</span></code> 。<code class="docutils literal notranslate"><span class="pre">dynamic_axes_inputs</span></code> 必须指定，<code class="docutils literal notranslate"><span class="pre">dynamic_axes_outputs</span></code> 不必须指定。如果模型是多输入，则{‘input’:{0:
‘batch_size’}}的个数须配置与输入节点个数相等。</p>
<p>例如模型为两输入，设置如下</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;dynamic_axes_inputs&quot;</span><span class="p">:{</span><span class="s1">&#39;input0&#39;</span><span class="p">:{</span><span class="mi">0</span><span class="p">:</span><span class="s1">&#39;batch_size&#39;</span><span class="p">},</span>
<span class="s1">&#39;input1&#39;</span><span class="p">:{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">}}</span>
</pre></div>
</div>
<p>若仅转换模型，则执行以下命令</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>resnet18<span class="w"> </span>-f<span class="w"> </span>pytorch<span class="w"> </span>-r<span class="w"> </span>convert
-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_pytorch/resnet18-5c106cde.pth
--save-dir<span class="w"> </span>result<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>torch.onnx.export</strong></p></li>
</ol>
<p>基于Pytorch2.1，官方转出onnx接口如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">opset_version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>这里面的model为原始模型定义，args是类型为torch.Tensor的模型输入，f指定要转出onnx模型的文件路径，opset_version指定要转出onnx的算子集版本，这里优先指定为14，若已转出的onnx算子集版本小于或大于14，可以使用官方提供的版本转换工具升级或降级到版本14；input_names/output_names分别为网络的输入输出名（可缺省）。官方接口和示例文档分别参见<a class="reference external" href="https://pytorch.org/docs/2.1/onnx_torchscript.html#torch.onnx.export">torch.onnx.export</a>和<a class="reference external" href="https://pytorch.org/docs/2.1/onnx_torchscript.html#example-alexnet-from-pytorch-to-onnx">tutorial</a>。</p>
</section>
<section id="id22">
<h4><span class="section-number">4.6.2.2. </span>定义前向推理函数<a class="headerlink" href="#id22" title="Permalink to this heading"></a></h4>
<p><a class="reference internal" href="#id11">ONNX定义前向推理函数</a></p>
</section>
<section id="id23">
<h4><span class="section-number">4.6.2.3. </span>执行量化命令进行量化<a class="headerlink" href="#id23" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>resnet18<span class="w"> </span>-f<span class="w"> </span>pytorch<span class="w"> </span>-if<span class="w"> </span>infer_cls_model
--save-dir<span class="w"> </span>result<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_pytorch/resnet18-5c106cde.pth
-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
</section>
</section>
<section id="qat">
<h3><span class="section-number">4.6.3. </span>量化QAT模型<a class="headerlink" href="#qat" title="Permalink to this heading"></a></h3>
<p>量化由清微Finetune-Lib库训练得到的QAT模型，用户需提供训练得到的qdq模型，由ONNX量化工具加载并量化，以期获得精度更高的量化后模型。</p>
<section id="id24">
<h4><span class="section-number">4.6.3.1. </span>定义前向推理函数<a class="headerlink" href="#id24" title="Permalink to this heading"></a></h4>
<p><a class="reference internal" href="#id11">ONNX定义前向推理函数</a></p>
</section>
<section id="id25">
<h4><span class="section-number">4.6.3.2. </span>执行量化命令进行量化<a class="headerlink" href="#id25" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_qdq/resnet18_qdq.onnx
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>--save-dir<span class="w"> </span>result<span class="w"> </span>-uds<span class="w">  </span>/opt/Quantize/onnx_examples/infer_demo.py<span class="w">                             </span><span class="p">|</span>
</pre></div>
</div>
</section>
</section>
<section id="id26">
<h3><span class="section-number">4.6.4. </span>约束条件<a class="headerlink" href="#id26" title="Permalink to this heading"></a></h3>
<p>模型转化中有以下场景不支持：</p>
<p>Pytorch到ONNX模型的转换基于torch.fx实现，如下几种情况不支持：</p>
<p>1)前向函数定义含有动态控制流。例如循环、判断语句并且其条件执行依赖于输入数据。</p>
<p>2)前向函数含有python内置函数。例如len()、all()、assert和其它python的外部函数工具包等。</p>
<p>3)前向函数含有&#64;运算符，需用torch.matmul替换。</p>
<p>4)前向函数不支持tensor.to(torch.device(‘cpu’)运算。</p>
<p>5)针对QAT模型，在训练和推理阶段，待转换模型结构不一致。例如在GoogleNet网络定义中，self.aux1模块只在训练阶段出现，不参与前向推理。具体定义如下所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inception4a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">aux1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="n">aux1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inception4b</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>6)前向函数中含有if判断且判断条件为bool变量时，需在转换函数中定义concrete_args</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">yolov5_v7_relu</span><span class="p">(</span><span class="n">weight_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">weight_path</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">attempt_load</span><span class="p">(</span><span class="n">weight_path</span><span class="p">)</span>
    <span class="n">concrete_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;augment&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;profile&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;visualize&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span><span class="s2">&quot;val&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
    <span class="n">in_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
            <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">))],</span>
            <span class="s2">&quot;concrete_args&quot;</span><span class="p">:</span> <span class="n">concrete_args</span>
             <span class="p">}</span>

<span class="k">return</span> <span class="n">in_dict</span>
</pre></div>
</div>
<p>7)前向函数中带有torch.reshape但入参不是固定shape时，需改为torch.view()</p>
<p>8)前向中含有torch.mul_等inplace操作时，需改为赋值写法。如：output=output.mul_(2)</p>
</section>
</section>
<section id="caffe">
<h2><span class="section-number">4.7. </span>Caffe模型量化<a class="headerlink" href="#caffe" title="Permalink to this heading"></a></h2>
<p>Caffe模型量化主体流程如下图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_19.png" />
</figure>
<p></p>
<section id="caffe-demo">
<h3><span class="section-number">4.7.1. </span>caffe demo量化<a class="headerlink" href="#caffe-demo" title="Permalink to this heading"></a></h3>
<p>工具包中提供示例demo，使用命令行如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>-ch<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-f<span class="w"> </span>caffe<span class="w"> </span>-if<span class="w"> </span>infer_cls_model
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_caffe/resnet-18.prototxt
-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_caffe/resnet-18.caffemodel
--save-dir<span class="w"> </span>result<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示量化成功:</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_20.png" />
</figure>
<p></p>
</section>
<section id="id27">
<h3><span class="section-number">4.7.2. </span>量化新模型<a class="headerlink" href="#id27" title="Permalink to this heading"></a></h3>
<section id="id28">
<h4><span class="section-number">4.7.2.1. </span>模型转换<a class="headerlink" href="#id28" title="Permalink to this heading"></a></h4>
<p>首先，需要确认Caffe模型的算子符合 <a class="reference internal" href="../op/op.html"><span class="doc">算子支持列表</span></a> 。</p>
<p>Caffe模型结构文件和模型权重文件，分别由传入参数–model和–weight指定。转出的ONNX模型保存在{–save-dir}/的目录下。转出的ONNX模型名称由原Caffe模型结构文件名和后缀.onnx组成。例如，demo实例中–model为resnet18.prototxt，则转出的模型命名为resnet18.onnx。当所转出的ONNX模型和Caffe原始模型浮点比对一致后，则转换成功，且在窗口打印提示信息。</p>
<p>模型转换示例命令行：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>-ch<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-f<span class="w"> </span>caffe<span class="w"> </span>-if<span class="w"> </span>infer_cls_model
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_caffe/resnet-18.prototxt
-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_caffe/resnet-18.caffemodel
--save-dir<span class="w"> </span>result<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py<span class="w"> </span>-r<span class="w"> </span>convert
</pre></div>
</div>
<p>上述执行后，屏幕打印如下信息则表示模型转换成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_21.png" />
</figure>
<p></p>
</section>
<section id="id29">
<h4><span class="section-number">4.7.2.2. </span>定义前向推理函数<a class="headerlink" href="#id29" title="Permalink to this heading"></a></h4>
<p><a class="reference internal" href="#id11">ONNX定义前向推理函数</a></p>
</section>
<section id="id30">
<h4><span class="section-number">4.7.2.3. </span>执行量化命令进行量化<a class="headerlink" href="#id30" title="Permalink to this heading"></a></h4>
<blockquote>
<div><p><a class="reference internal" href="#caffe-demo">caffe demo量化</a></p>
</div></blockquote>
</section>
</section>
</section>
<section id="paddlepaddle">
<h2><span class="section-number">4.8. </span>PaddlePaddle模型量化<a class="headerlink" href="#paddlepaddle" title="Permalink to this heading"></a></h2>
<p>PaddlePaddle模型量化主体流程如下图所示：（以下简称PaddlePaddle为paddle）</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_22.png" />
</figure>
<section id="demo-2">
<span id="id31"></span><h3><span class="section-number">4.8.1. </span>demo量化<a class="headerlink" href="#demo-2" title="Permalink to this heading"></a></h3>
<p>示例一：量化常规paddle模型</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-f<span class="w"> </span>paddle<span class="w"> </span>-if<span class="w"> </span>infer_cls_model
--save-dir<span class="w"> </span>result<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_paddle/resnet18.pdiparams
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_paddle/resnet18.pdmodel
-is<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">224</span><span class="w"> </span><span class="m">224</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_23.png" />
</figure>
<p></p>
</section>
<section id="id32">
<h3><span class="section-number">4.8.2. </span>量化新模型<a class="headerlink" href="#id32" title="Permalink to this heading"></a></h3>
<section id="id33">
<h4><span class="section-number">4.8.2.1. </span>模型转换<a class="headerlink" href="#id33" title="Permalink to this heading"></a></h4>
<p>在对paddle模型量化之前，需要进行模型转换即将paddle模型转换为ONNX模型，组成paddle模型的算子集合须符合 <a class="reference internal" href="../op/op.html"><span class="doc">算子支持列表</span></a> 。</p>
<p>目前该工具只支持加载静态的paddle模型进行转换。静态模型是paddle官方推荐的用于推理部署的形态，通常由paddle.jit.save()或paddle.static.save_inference_model()或paddle.fluid.io.save_inference_model()生成保存。生成结果一般包含3个文件，形如resnet18.pdmodel、resnet18.pdiparams、resnet18.pdiparams.info；其中带pdmodel后缀表示模型文件，带pdiparams后缀表示参数文件，带info后缀是对参数补充说明的文件，此处不需要用到该文件。在使用上述3个函数保存静态推理模型时，也可自定义模型或参数的保存文件名，如__model__、__params__，但是建议使用规范的“模型名+相应后缀”的命名方式
。</p>
<p>使用该工具转换静态模型时，需要指定-m(–model)和-w(–weight)参数，-m参数为模型文件（如上述resnet18.pdmodel或者__model__）的具体路径，-w参数为参数文件（如上述resnet18.pdiparams或者__params__）的具体路径。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意：模型文件和参数文件需要保存在同一个目录下；同时建议转换开始前使用合适的名字命名模型文件和参数文件，如模型名字+相应后缀，参考demo中的resnet18.pdmodel/resnet18.pdiparams。
常见的静态模型的输入shape的各个维度均是固定的，或者只有第零维可变，如(128,3,224,224)、(-1,3,224,224)；也有少数静态模型的输入存在多个维度可动态赋值的情况，如(10,3,-1,-1)，需要为后两个维度指定具体数值；用户可在命令行中通过-is(–input-shapes)指定输入，如–input-shapes
10 3 224 224。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注意：–input-shapes只是用来处理静态模型输入shape的某些维度是动态（即-1或None）的情况；不能通过-is参数更改静态模型非动态输入维度，如静态模型输入shape为(10,3,-1,-1)，则不能设置–input-shapes
10 9 224 224 或者8 3 224 224；目前不支持第一维的3-&gt;9或者第零维的10-&gt;8这种改动。</p>
</div>
<p>当前某些模型输入shape的batchsize维度如果设置为-1（如输入shape为[-1,3,224,224]，-1表示batchsize），量化可能会失败。此时请指定具体的batchsize进行尝试，如[4,3,224,224]。我们将在后续版本优化该问题。</p>
<p>对于有多个输入的，请以相同方式按顺序指定（中间如果有其他非tensor类型的跳过即可）。
执行以下命令转换：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-f<span class="w"> </span>paddle<span class="w"> </span>-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-r<span class="w"> </span>convert
--save-dir<span class="w"> </span>result<span class="w"> </span>-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_paddle/resnet18.pdiparams
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_paddle/resnet18.pdmodel
-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>上述执行后，屏幕打印如下信息则表示模型转换成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_24.png" />
</figure>
<p></p>
</section>
<section id="id34">
<h4><span class="section-number">4.8.2.2. </span>定义前向推理函数<a class="headerlink" href="#id34" title="Permalink to this heading"></a></h4>
<p><a class="reference internal" href="#id11">ONNX定义前向推理函数</a></p>
</section>
<section id="id35">
<h4><span class="section-number">4.8.2.3. </span>执行量化命令进行量化<a class="headerlink" href="#id35" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-f<span class="w"> </span>paddle<span class="w"> </span>-if<span class="w"> </span>infer_cls_model
--save-dir<span class="w"> </span>result<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-w<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_paddle/resnet18.pdiparams
-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_paddle/resnet18.pdmodel
-is<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">224</span><span class="w"> </span><span class="m">224</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>说明：-bs指定了量化过程使用数据的batch大小；-is指定转换生成的onnx的输入shape（默认第零维为batch大小）；要求-bs和-is指定的batch大小要一致；如果转换前的paddle网络的输入shape包含动态batch（batch=-1），则可以不用指定-is。</p>
</section>
</section>
</section>
<section id="tensorflow">
<h2><span class="section-number">4.9. </span>Tensorflow模型量化<a class="headerlink" href="#tensorflow" title="Permalink to this heading"></a></h2>
<p>Tensorflow模型量化主体流程如下图所示：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_25.png" />
</figure>
<p></p>
<section id="id36">
<h3><span class="section-number">4.9.1. </span>demo量化示例<a class="headerlink" href="#id36" title="Permalink to this heading"></a></h3>
<p>步骤一：量化模型</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_tf/resnet18.pb
-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-f<span class="w"> </span>tensorflow<span class="w"> </span>-snn<span class="w"> </span>data_input:0<span class="w"> </span>-enn<span class="w"> </span>output:0
--save-dir<span class="w"> </span>result<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息则表示量化成功：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_26.png" />
</figure>
<p></p>
</section>
<section id="id37">
<h3><span class="section-number">4.9.2. </span>量化新模型<a class="headerlink" href="#id37" title="Permalink to this heading"></a></h3>
<section id="id38">
<h4><span class="section-number">4.9.2.1. </span>模型转换<a class="headerlink" href="#id38" title="Permalink to this heading"></a></h4>
<p>Tensorflow模型需要首先转换为ONNX模型后才能进行量化。</p>
<p>首先，需要确认Tensorflow模型的算子符合 <a class="reference internal" href="../op/op.html"><span class="doc">算子支持列表</span></a>  约束。</p>
<p>Tensorflow模型由传入参数–model指定。转出的ONNX模型保存在{–save-dir}/目录下。转出的ONNX模型名称由原Tensorflow模型文件名和后缀.onnx组成。例如，demo实例中–model为resnet18.pb，则转出的模型命名为resnet18.onnx。当所转出的ONNX模型和Tensorflow原始模型浮点比对一致后，则转换成功。</p>
<p>执行如下命令转换模型，示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m<span class="w"> </span>/opt/Quantize/onnx_examples/model/resnet18_tf/resnet18.pb
-if<span class="w"> </span>infer_test_float<span class="w"> </span>-f<span class="w"> </span>tensorflow<span class="w"> </span>-snn<span class="w"> </span>data_input:0<span class="w"> </span>-enn<span class="w"> </span>output:0
--save-dir<span class="w"> </span>result<span class="w"> </span>-r<span class="w"> </span>convert<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行完成后，生成resnet18.onnx模型文件：</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_27.png" />
</figure>
<p></p>
</section>
<section id="id39">
<h4><span class="section-number">4.9.2.2. </span>定义前向推理函数<a class="headerlink" href="#id39" title="Permalink to this heading"></a></h4>
<p><a class="reference internal" href="#id11">ONNX定义前向推理函数</a></p>
</section>
<section id="id40">
<h4><span class="section-number">4.9.2.3. </span>执行量化命令进行量化<a class="headerlink" href="#id40" title="Permalink to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>-m
/opt/Quantize/onnx_examples/model/resnet18_tf/resnet18.pb<span class="w"> </span>-if
infer_cls_model<span class="w"> </span>-f<span class="w"> </span>tensorflow<span class="w"> </span>-snn<span class="w"> </span>data_input:0<span class="w"> </span>-enn<span class="w"> </span>output:0<span class="w"> </span>--save-dir
result<span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
</section>
</section>
</section>
<section id="id41">
<h2><span class="section-number">4.10. </span>量化后模型路径<a class="headerlink" href="#id41" title="Permalink to this heading"></a></h2>
<p>存放位置：{–save-dir}/目录下</p>
<p>文件名称：{原浮点模型名称}_quantize.&lt;扩展名&gt;</p>
<p>例如：</p>
<blockquote>
<div><p>原文件名称：<code class="docutils literal notranslate"><span class="pre">resnet18.onnx</span></code></p>
<p>量化后名称：<code class="docutils literal notranslate"><span class="pre">resnet18_quantize.onnx</span></code></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>量化过程中生成的中间文件也放置在{–save-dir}/目录下，其中dump目录下存放模型各层输出结果文件，steps目录下存放量化过程中各阶段生成的中间模型。</p>
</div>
</section>
<section id="id42">
<h2><span class="section-number">4.11. </span>测试量化后模型精度<a class="headerlink" href="#id42" title="Permalink to this heading"></a></h2>
<p>量化模型的精度测试需要在完整的测试数据集上进行。为了方便用户，我们也提供了该项功能，使用该功能时用户要在参数-if对应的infer-func函数里加载完整测试集，并根据模型预测结果计算相对应的精度，需要注意的是参数-i指定的数字需要能够覆盖整个测试集，参数-r设置为infer表示推理模式，参数–model需指定为量化后的模型，示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Knight<span class="w"> </span>--chip<span class="w"> </span>TX5368AV200<span class="w"> </span>quant<span class="w"> </span>--save-dir<span class="w"> </span>result<span class="w"> </span>-m
/result/resnet18_quantize.onnx<span class="w"> </span>-if<span class="w"> </span>infer_cls_model<span class="w"> </span>-qm<span class="w"> </span>kl<span class="w"> </span>-bs<span class="w"> </span><span class="m">10</span><span class="w"> </span>-r
infer<span class="w"> </span>-i<span class="w"> </span><span class="m">100</span><span class="w"> </span>-uds<span class="w"> </span>/opt/Quantize/onnx_examples/infer_demo.py
</pre></div>
</div>
<p>执行成功后，屏幕会打印如下信息:</p>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_28.png" />
</figure>
<p></p>
</section>
<section id="id43">
<h2><span class="section-number">4.12. </span>注意事项<a class="headerlink" href="#id43" title="Permalink to this heading"></a></h2>
<p>用户需注意以下事项：</p>
<ol class="arabic simple">
<li><p>模型中需要量化的算子需要包含在 <a class="reference internal" href="../op/op.html"><span class="doc">算子支持列表</span></a> 中。</p></li>
<li><p>在进行多进程量化模型时不同进程对应的–save-dir存储目录必须不同，若相同则可能导致量化异常。</p></li>
<li><p>转换为ONNX模型时需使用指定版本的ONNX(1.14.0)进行转换。</p></li>
<li><p>ONNX浮点模型输入只支持float32数据类型，不支持其它输入数据类型。</p></li>
<li><p>ONNX量化后模型(未增加输出反量化)输出结果为浮点类型表示的定点结果数据。</p></li>
<li><p>Compare工具支持比对的算子与 <a class="reference internal" href="../op/op.html"><span class="doc">算子支持列表</span></a> 中支持算子保持一致。</p></li>
<li><p>ONNX量化前浮点模型支持静态图和动态图，量化后定点模型只支持静态图。</p></li>
<li><p>若用户设置的PYTHONPATH中存在与ONNX量化工具下定义的包同名时，可能会引起包引用异常。</p></li>
<li><p>ONNX静态图是指输入维度均为固定值的图(如下左图所示)，动态图是指 <code class="docutils literal notranslate"><span class="pre">batchsize</span></code> 可配置的图(如下右图所示)。</p></li>
</ol>
<figure class="align-center">
<img alt="pipeline" src="../_images/quant_29.png" />
</figure>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../quick_demo/quick_demo.html" class="btn btn-neutral float-left" title="3. 快速上手指南" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="compile.html" class="btn btn-neutral float-right" title="5. 编译仿真性能分析使用指南" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright COPYRIGHT© 2024北京清微智能科技有限公司, 保留所有权利。.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>